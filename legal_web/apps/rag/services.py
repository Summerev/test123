# teamproject/legal_web/apps/rag/services.py

from openai import OpenAI
from django.conf import settings

from . import retriever, prompt_manager

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (settings.pyì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨)
client = OpenAI(api_key=settings.OPENAI_API_KEY)

# --- ë¬¸ì„œ ë¶„ì„ ì„œë¹„ìŠ¤ ---
def analyze_document(uploaded_file, doc_type, language='ko'):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ìš”ì•½ë¬¸ê³¼ ê²€ìƒ‰ì„ ìœ„í•œ ë°ì´í„°(ì¸ë±ìŠ¤, ì²­í¬)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” DBì™€ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
    """
    try:
        # 1. íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        document_text = retriever.get_document_text(uploaded_file)
        if not document_text:
            raise ValueError("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        doc_type_name = "ì´ìš©ì•½ê´€" if doc_type == "terms" else "ê³„ì•½ì„œ"

        # 2. Map-Reduce ë°©ì‹ìœ¼ë¡œ ìš”ì•½ ìƒì„±
        # (ë¬¸ì„œê°€ ì§§ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë¡œì§ì€ ê°„ì†Œí™”í•˜ê³ , Map-Reduceì— ì§‘ì¤‘)
        summary_chunks = retriever.split_text_into_chunks(document_text, max_tokens=2000)
        
        # Map ë‹¨ê³„: ê° ì²­í¬ ìš”ì•½
        individual_summaries = []
        for chunk in summary_chunks:
            prompt = prompt_manager.get_summarize_chunk_prompt(chunk, doc_type_name)
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.3,
            )
            individual_summaries.append(response.choices[0].message.content)
            
        # Reduce ë‹¨ê³„: ìš”ì•½ë³¸ ì¢…í•©
        final_summary_prompt = prompt_manager.get_combine_summaries_prompt(individual_summaries, doc_type_name)
        final_summary_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": final_summary_prompt}],
            max_tokens=1000,
            temperature=0.7,
        )
        final_summary = final_summary_response.choices[0].message.content

        # 3. ìœ„í—˜ ìš”ì†Œ ì‹ë³„
        risk_prompt = prompt_manager.get_risk_factors_prompt(document_text, language)
        risk_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": risk_prompt}],
            max_tokens=1000,
            temperature=0.3,
        )
        risk_text = risk_response.choices[0].message.content
        
        # 4. ê²€ìƒ‰ì„ ìœ„í•œ FAISS ì¸ë±ìŠ¤ ìƒì„±
        qa_chunks = retriever.split_text_into_chunks(document_text, max_tokens=500) # QAìš©ì€ ë” ì˜ê²Œ
        faiss_index, indexed_chunks = retriever.create_faiss_index(client, qa_chunks)

        # 5. ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        return {
            "success": True,
            "summary": f"ğŸ“‹ ë¬¸ì„œ ìš”ì•½\n\n{final_summary}\n\n---\n\nâš ï¸ ìœ„í—˜ ìš”ì†Œ ì‹ë³„\n\n{risk_text}",
            "faiss_index": faiss_index,
            "chunks": indexed_chunks,
        }

    except Exception as e:
        print(f"Error in analyze_document service: {e}")
        return {"success": False, "error": str(e)}


# --- ì§ˆì˜ì‘ë‹µ ì„œë¹„ìŠ¤ ---
def get_answer(question, faiss_index, chunks, chat_history=[]):
    """
    ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ ë°ì´í„°(ì¸ë±ìŠ¤, ì²­í¬)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        # 1. FAISSì—ì„œ ê´€ë ¨ ë¬¸ì„œ ì¡°ê° ê²€ìƒ‰
        relevant_chunks = retriever.search_faiss_index(faiss_index, chunks, client, question)
        context = "\n\n".join(relevant_chunks)
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
        prompt = prompt_manager.get_answer_prompt(context, question)
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ messagesì— í¬í•¨
        messages = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."}]
        for entry in chat_history:
            role = "user" if entry.get("sender") == "user" else "assistant"
            messages.append({"role": role, "content": entry.get("text")})
        messages.append({"role": "user", "content": prompt})
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=500,
            temperature=0.5,
        )
        answer = response.choices[0].message.content
        
        return {"success": True, "answer": answer}

    except Exception as e:
        print(f"Error in get_answer service: {e}")
        return {"success": False, "error": str(e)}