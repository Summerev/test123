# apps/rag/services/rag_engine.py
import re
from typing import List, Dict, Any, Optional, Tuple
from .qdrant_client import QdrantVectorStore, EmbeddingService
from .document_processor import DocumentProcessor, CONTRACT_TYPES_TERMS

class RAGEngine:
    """ê°•í™”ëœ RAG ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self):
        self.vector_store = QdrantVectorStore()
        self.embedding_service = EmbeddingService()
        self.document_chunks = []
        self.keyword_index = {}
        self.detected_contract_type = None
        
    def process_document(self, text: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        # 1. ê³„ì•½ì„œ ìœ í˜• ê°ì§€
        self.detected_contract_type, confidence, type_info = DocumentProcessor.detect_contract_type(text)
        
        # 2. ì¡°í•­ë³„ ì¶”ì¶œ
        self.document_chunks = DocumentProcessor.extract_articles_with_content(text)
        
        # 3. í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„±
        self.keyword_index = self._create_enhanced_keyword_index(self.document_chunks, self.detected_contract_type)
        
        # 4. ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
        vector_success = self._create_vector_index(self.document_chunks)
        
        return {
            'contract_type': self.detected_contract_type,
            'confidence': confidence,
            'chunk_count': len(self.document_chunks),
            'vector_index_created': vector_success,
            'keyword_groups': len(self.keyword_index)
        }
    
    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """ê°•í™”ëœ RAG ê²€ìƒ‰"""
        print(f"\nğŸ” ê°•í™”ëœ RAG ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # 1. ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì‚¬
        is_relevant, reason = self._enhanced_strict_document_relevance_check(query)
        if not is_relevant:
            print(f"âŒ ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {reason}")
            return []
        
        print(f"âœ… ë¬¸ì„œ ê´€ë ¨ì„± í™•ì¸: {reason}")
        
        all_matches = []
        
        # 2. ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ê²€ìƒ‰ (ìµœìš°ì„ )
        article_matches = self._search_specific_articles(query)
        if article_matches:
            print(f"ğŸ“‹ ì¡°í•­ ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ: {len(article_matches)}ê°œ ì¡°í•­ ë°œê²¬")
            for match in article_matches:
                all_matches.append({
                    'chunk': match,
                    'score': 100,
                    'method': f'ì¡°í•­ê²€ìƒ‰(ì œ{match["article_num"]}ì¡°)',
                    'index': match.get('index', 0)
                })
        
        # 3. ë¬¸ì„œ ë‚´ í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­
        exact_matches = self._search_exact_keywords(query)
        if exact_matches:
            print(f"ğŸ¯ ë¬¸ì„œ ë‚´ í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­: {len(exact_matches)}ê°œ")
            for match in exact_matches:
                all_matches.append({
                    'chunk': match['chunk'],
                    'score': 80 + (match['total_count'] * 2),
                    'method': f'ì •í™•ë§¤ì¹­({match["keyword"]}:{match["total_count"]}íšŒ)',
                    'index': match['index']
                })
        
        # 4. ê³„ì•½ì„œ ìœ í˜•ë³„ íŠ¹í™” ê²€ìƒ‰
        if self.detected_contract_type and self.detected_contract_type in CONTRACT_TYPES_TERMS:
            specialized_terms = CONTRACT_TYPES_TERMS[self.detected_contract_type]
            query_lower = query.lower()
            
            for term in specialized_terms:
                if term in query_lower:
                    for i, chunk in enumerate(self.document_chunks):
                        if term in chunk['text']:
                            all_matches.append({
                                'chunk': chunk,
                                'score': 70,
                                'method': f'íŠ¹í™”ê²€ìƒ‰({term})',
                                'index': i
                            })
                            print(f"ğŸ¯ íŠ¹í™” ê²€ìƒ‰ ë§¤ì¹­: {term}")
        
        # 5. í‚¤ì›Œë“œ ê·¸ë£¹ ê²€ìƒ‰
        keyword_mapping = {
            'í•´ì§€_ì¢…ë£Œ': ['í•´ì§€', 'í•´ì œ', 'ì¢…ë£Œ', 'ì¤‘ë‹¨', 'ëë‚´', 'íŒŒê¸°'],
            'ì˜ë¬´_ì±…ì„': ['ì˜ë¬´', 'ì±…ì„', 'ì´í–‰', 'ì¤€ìˆ˜', 'ì™„ìˆ˜'],
            'ì†í•´_ë°°ìƒ': ['ì†í•´', 'ë°°ìƒ', 'í”¼í•´', 'ë³´ìƒ', 'ë³€ìƒ'],
            'ì§€ê¸‰_ê²°ì œ': ['ì§€ê¸‰', 'ì§€ë¶ˆ', 'ë‚©ë¶€', 'ê²°ì œ', 'ëˆ', 'ë¹„ìš©', 'ëŒ€ê¸ˆ'],
            'ê¸°ê°„_ê¸°í•œ': ['ê¸°ê°„', 'ê¸°í•œ', 'ì¼ì', 'ë‚ ì§œ', 'ì‹œì ', 'ì‹œê¸°'],
        }
        
        query_lower = query.lower()
        for keyword_group, search_keywords in keyword_mapping.items():
            matched_keywords = [kw for kw in search_keywords if kw in query_lower]
            if matched_keywords and keyword_group in self.keyword_index:
                for match in self.keyword_index[keyword_group][:1]:
                    chunk = self.document_chunks[match['chunk_idx']]
                    all_matches.append({
                        'chunk': chunk,
                        'score': match['score'] + 30,
                        'method': f'í‚¤ì›Œë“œê·¸ë£¹({keyword_group})',
                        'index': match['chunk_idx']
                    })
                    print(f"ğŸ” í‚¤ì›Œë“œ ê·¸ë£¹ ë§¤ì¹­: {keyword_group}")
        
        # 6. ë²¡í„° ê²€ìƒ‰ (ìµœí›„ ìˆ˜ë‹¨)
        if len(all_matches) < 2:
            print("ğŸ”¢ ë²¡í„° ê²€ìƒ‰ ì‹œë„ (ìµœí›„ ìˆ˜ë‹¨)")
            vector_matches = self._perform_vector_search(query, top_k)
            all_matches.extend(vector_matches)
        else:
            print("âœ… ì¶©ë¶„í•œ ë§¤ì¹­ ê²°ê³¼ë¡œ ë²¡í„° ê²€ìƒ‰ ìƒëµ")
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
        unique_results = {}
        for match in all_matches:
            chunk_idx = match['index']
            if chunk_idx not in unique_results or unique_results[chunk_idx]['score'] < match['score']:
                unique_results[chunk_idx] = match
        
        final_results = sorted(unique_results.values(), key=lambda x: x['score'], reverse=True)[:top_k]
        
        print(f"ğŸ“Š ìµœì¢… RAG ê²€ìƒ‰ ê²°ê³¼: {len(final_results)}ê°œ ì¡°í•­ ì„ íƒ")
        for i, result in enumerate(final_results):
            chunk = result['chunk']
            if isinstance(chunk, dict):
                article_info = f"ì œ{chunk.get('article_num', '?')}ì¡°({chunk.get('article_title', 'Unknown')})"
            else:
                article_info = f"ì œ{chunk.article_num}ì¡°({chunk.article_title})" if hasattr(chunk, 'article_num') else "Unknown"
            print(f"   {i+1}. {article_info} - {result['method']} (ì ìˆ˜: {result['score']:.1f})")
        
        return final_results
    
    def _enhanced_strict_document_relevance_check(self, query: str) -> Tuple[bool, str]:
        """ê°•í™”ëœ ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì‚¬"""
        print(f"ğŸ” ê°•í™”ëœ ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì‚¬: '{query}'")
        
        query_lower = query.lower().strip()
        
        # 1. ëª…ë°±íˆ ë¬¸ì„œì™€ ë¬´ê´€í•œ ì§ˆë¬¸ë“¤
        off_topic_patterns = [
            r'(ë‚ ì”¨|ê¸°ìƒ|ì˜¨ë„|ë¹„|ëˆˆ|ë§‘ìŒ|íë¦¼)',
            r'(ë‰´ìŠ¤|ì‹ ë¬¸|ë°©ì†¡|ì–¸ë¡ |ê¸°ì‚¬)',
            r'(ìš”ë¦¬|ìŒì‹|ì‹ë‹¹|ë§›ì§‘|ë ˆì‹œí”¼)',
            r'(ì˜í™”|ë“œë¼ë§ˆ|ë…¸ë˜|ìŒì•…|ì—”í„°í…Œì¸ë¨¼íŠ¸)',
            r'(ê²Œì„|ìŠ¤í¬ì¸ |ì¶•êµ¬|ì•¼êµ¬|ë†êµ¬)',
            r'(ì •ì¹˜|ì„ ê±°|ëŒ€í†µë ¹|êµ­íšŒ|ì •ë‹¹)',
            r'(ì•ˆë…•|ìƒì¼|ë‚˜ì´|ê²°í˜¼|ê°€ì¡±)',
            r'(ì—¬í–‰|íœ´ê°€|ê´€ê´‘|í˜¸í…”|í•­ê³µ)',
            r'(ê±´ê°•|ë³‘ì›|ì˜ì‚¬|ì•½|ì¹˜ë£Œ)',
            r'(í•™êµ|ê³µë¶€|ì‹œí—˜|ì„±ì |êµìœ¡)',
            r'(ì»´í“¨í„°|í”„ë¡œê·¸ë˜ë°|ì¸í„°ë„·|ì†Œí”„íŠ¸ì›¨ì–´)',
            r'(ìˆ˜í•™|ê³¼í•™|ë¬¼ë¦¬|í™”í•™|ìƒë¬¼)',
            r'(ì—­ì‚¬|ì§€ë¦¬|ë¬¸í™”|ì˜ˆìˆ |ì² í•™)',
            r'(ë„ˆëŠ”|ë‹¹ì‹ ì€|aiëŠ”|ì¸ê³µì§€ëŠ¥)',
            r'(ì–´ë–»ê²Œ|ì™œ|ì–¸ì œ) (ìƒê°|ëŠë¼|íŒë‹¨)',
            r'(ì¢‹ì•„í•˜|ì‹«ì–´í•˜|ì„ í˜¸í•˜|ì¶”ì²œ)',
            r'^(ì•ˆë…•|hello|hi|í—¬ë¡œ|í•˜ì´)$',
            r'^(ê³ ë§ˆì›Œ|ê°ì‚¬|thanks|thank you)$',
            r'^(ì‘|ë„¤|ì˜ˆ|yes|no|ì•„ë‹ˆì˜¤)$',
            r'(ëª‡ì‹œ|ì–¸ì œ|ì§€ê¸ˆ|ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼)',
            r'(ì‹œê°„|ì‹œê°|ë‚ ì§œ|ìš”ì¼)',
            r'(ì–´ë–»ê²Œ í•´|ë°©ë²•|í•˜ëŠ”ë²•)(?!.*ê³„ì•½)',
        ]
        
        for pattern in off_topic_patterns:
            if re.search(pattern, query_lower):
                print(f"âŒ ë¬¸ì„œ ë¬´ê´€ íŒ¨í„´ ê°ì§€: {pattern}")
                return False, f"off_topic_pattern: {pattern}"
        
        # 2. ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ì§ˆë¬¸
        if len(query.strip()) < 2:
            print("âŒ ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŒ")
            return False, "too_short"
        
        if query_lower in ['?', '??', '???', '.', '..', '...', 'ã…', 'ã…‹', 'ã… ']:
            print("âŒ ì˜ë¯¸ì—†ëŠ” ì§ˆë¬¸")
            return False, "meaningless"
        
        # 3. ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ì–¸ê¸‰ (ìµœê³  ìš°ì„ ìˆœìœ„)
        article_patterns = [
            r'ì œ\s*\d+\s*ì¡°',
            r'\d+\s*ì¡°',
            r'ì œ\s*\d+',
            r'ì¡°í•­\s*\d+',
            r'article\s*\d+',
            r'section\s*\d+',
            r'clause\s*\d+'
        ]
        
        for pattern in article_patterns:
            if re.search(pattern, query_lower):
                print(f"âœ… ì¡°í•­ íŒ¨í„´ ê°ì§€: {pattern} (ìµœê³  ìš°ì„ ìˆœìœ„)")
                return True, f"article_pattern: {pattern}"
        
        # 4. ê³„ì•½ì„œ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ ì²´í¬
        contract_keywords = [
            'ê³„ì•½', 'ì¡°í•­', 'ì¡°', 'í•­', 'ê³„ì•½ì„œ', 'í˜‘ì•½', 'ì•½ì •', 'í˜‘ì •',
            'ë‹¹ì‚¬ì', 'ë°œì£¼ì', 'ìˆ˜ê¸‰ì¸', 'ì„ëŒ€ì¸', 'ì„ì°¨ì¸', 'ê°‘', 'ì„',
            'ì˜ë¬´', 'ì±…ì„', 'ê¶Œë¦¬', 'ê¶Œí•œ', 'ì´í–‰', 'ì¤€ìˆ˜', 'ì™„ìˆ˜',
            'ìœ„ë°˜', 'ìœ„ë°°', 'ë¶ˆì´í–‰', 'ë¯¸ì´í–‰', 'ì–´ê¹€',
            'ëŒ€ê¸ˆ', 'ë¹„ìš©', 'ìš”ê¸ˆ', 'ìˆ˜ìˆ˜ë£Œ', 'ë³´ì¦ê¸ˆ', 'ê³„ì•½ê¸ˆ', 'ì”ê¸ˆ',
            'ìœ„ì•½ê¸ˆ', 'ì—°ì²´ë£Œ', 'ì§€ì²´ìƒê¸ˆ', 'ì†í•´ë°°ìƒ', 'ë°°ìƒ', 'ë³´ìƒ',
            'ê¸‰ì—¬', 'ì„ê¸ˆ', 'ì›”ê¸‰', 'ë³´ìˆ˜', 'ìˆ˜ë‹¹',
            'ê¸°ê°„', 'ê¸°í•œ', 'ì¼ì', 'ë‚ ì§œ', 'ì‹œì ', 'ì™„ë£Œ', 'ì¢…ë£Œ', 'ë§Œë£Œ',
            'ì—°ì¥', 'ê°±ì‹ ', 'ì—°ê¸°', 'ì§€ì—°',
            'í•´ì§€', 'í•´ì œ', 'ë³€ê²½', 'ìˆ˜ì •', 'ê°±ì‹ ', 'ì—°ì¥', 'íŒŒê¸°',
            'í†µì§€', 'ê³ ì§€', 'ì‹ ê³ ', 'ìŠ¹ì¸', 'í•©ì˜', 'ë™ì˜',
            'ì¡°ê±´', 'ê¸°ì¤€', 'ë°©ë²•', 'ì ˆì°¨', 'ê³¼ì •', 'ë‚´ìš©', 'ë²”ìœ„',
            'ì‚¬í•­', 'ì„¸ë¶€', 'êµ¬ì²´', 'ëª…ì‹œ', 'ê·œì •', 'ì •í•¨',
            'ë²•ì ', 'ë²•ë¥ ', 'ì†Œì†¡', 'ë¶„ìŸ', 'ì¤‘ì¬', 'íŒê²°',
            'ê´€í• ', 'ì¤€ê±°ë²•', 'íš¨ë ¥', 'ë¬´íš¨',
            'ë‚©í’ˆ', 'ì¸ë„', 'ê²€ìˆ˜', 'í•˜ì', 'ë³´ì¦', 'ë‹´ë³´',
            'ë©´ì±…', 'ê·€ì±…', 'ê³¼ì‹¤', 'ê³ ì˜'
        ]
        
        # 5. ë¬¸ì„œì— ì‹¤ì œ í¬í•¨ëœ ë‹¨ì–´ í™•ì¸
        query_words = [word for word in query.split() if len(word) >= 2]
        document_text_lower = ' '.join([chunk['text'] for chunk in self.document_chunks]).lower()
        
        # ê³„ì•½ í‚¤ì›Œë“œ í™•ì¸
        found_keywords = [kw for kw in contract_keywords if kw in query_lower]
        
        # ë¬¸ì„œ ë‚´ ë‹¨ì–´ í™•ì¸
        found_in_document = []
        for word in query_words:
            if word in document_text_lower:
                found_in_document.append(word)
        
        # 6. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        relevance_score = 0
        relevance_score += len(found_keywords) * 10
        relevance_score += len(found_in_document) * 5
        
        # 7. ìµœì¢… íŒë‹¨
        is_relevant = False
        reason = ""
        
        if found_keywords:
            is_relevant = True
            reason = f"contract_keywords: {found_keywords[:3]}"
            print(f"âœ… ê³„ì•½ ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬: {found_keywords[:3]}")
        elif len(found_in_document) >= 2:
            is_relevant = True
            reason = f"document_words: {found_in_document[:3]}"
            print(f"âœ… ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë°œê²¬: {found_in_document[:3]}")
        else:
            print(f"âŒ ë¬¸ì„œì™€ ê´€ë ¨ì„± ë‚®ìŒ (í‚¤ì›Œë“œ: {len(found_keywords)}, ë¬¸ì„œë‹¨ì–´: {len(found_in_document)}, ì ìˆ˜: {relevance_score})")
            reason = f"low_relevance: keywords={len(found_keywords)}, doc_words={len(found_in_document)}"
        
        print(f"ğŸ” ê´€ë ¨ì„± ì ìˆ˜: {relevance_score}, íŒì •: {'ê´€ë ¨' if is_relevant else 'ë¬´ê´€'}")
        return is_relevant, reason
    
    def _search_specific_articles(self, query: str) -> List[Dict]:
        """ì¡°í•­ ë²ˆí˜¸ ì§ì ‘ ê²€ìƒ‰"""
        print(f"ğŸ¯ ì¡°í•­ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        patterns = [
            r'ì œ\s*(\d+)\s*ì¡°',
            r'(\d+)\s*ì¡°',
            r'ì œ\s*(\d+)\s*í•­',
            r'(\d+)\s*í•­',
            r'ì œ\s*(\d+)',
            r'ì¡°í•­\s*(\d+)',
            r'article\s*(\d+)',
            r'section\s*(\d+)',
            r'clause\s*(\d+)',
            r'paragraph\s*(\d+)'
        ]
        
        found_articles = []
        query_lower = query.lower()
        
        for pattern in patterns:
            matches = re.findall(pattern, query_lower)
            for match in matches:
                article_num = int(match)
                print(f"ğŸ” ì¡°í•­ ê²€ìƒ‰: ì œ{article_num}ì¡°")
                
                for i, chunk in enumerate(self.document_chunks):
                    if chunk.get('article_num') == article_num:
                        chunk_with_index = chunk.copy()
                        chunk_with_index['index'] = i
                        found_articles.append(chunk_with_index)
                        print(f"âœ… ì œ{article_num}ì¡° ë°œê²¬: {chunk['article_title']}")
                        break
                else:
                    print(f"âŒ ì œ{article_num}ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return found_articles
    
    def _search_exact_keywords(self, query: str) -> List[Dict]:
        """ë¬¸ì„œ ë‚´ í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­"""
        print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{query}'")
        
        exact_matches = []
        query_words = query.split()
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ í•„í„°ë§
        stop_words = ['ê·¸ê²Œ', 'ê·¸ê±´', 'ë­ì•¼', 'ë­”ê°€', 'ì–´ë–¤', 'ì–´ë””', 'ì–¸ì œ', 'ì™œ',
                      'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜ëŠ”', 'ì•„ë‹ˆë©´',
                      'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ê±°', 'ê·¸ê±°', 'ì €ê±°',
                      'ë¬´ì—‡', 'ì–´ëŠ', 'ëˆ„êµ¬', 'ì–´ë–»ê²Œ', 'ì–¼ë§ˆë‚˜',
                      'ì˜', 'ê°€', 'ì´', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
                      'ì€', 'ëŠ”', 'ê³¼', 'ì™€', 'ë„', 'ë¼', 'ë¼ì„œ']
        
        meaningful_words = [word for word in query_words
                           if len(word) >= 2 and word not in stop_words]
        
        print(f"ğŸ” ì˜ë¯¸ìˆëŠ” ê²€ìƒ‰ ë‹¨ì–´: {meaningful_words}")
        
        for word in meaningful_words:
            matches_for_word = []
            
            for i, chunk in enumerate(self.document_chunks):
                chunk_text = chunk['text']
                
                # ì •í™• ë§¤ì¹­
                exact_count = chunk_text.count(word)
                
                # ìœ ì‚¬ ë§¤ì¹­ (ì–´ê°„ ë³€í™” ê³ ë ¤)
                word_pattern = word + r'[ì€ëŠ”ì´ê°€ì„ë¥¼ì˜ì—ì„œë¡œë¶€í„°ê¹Œì§€ì™€ê³¼ë„ë‚˜ë©°ìœ¼ë¡œì¨ì—ê²Œì—ì„œë¶€í„°]*'
                similar_matches = len(re.findall(word_pattern, chunk_text))
                
                total_count = exact_count + similar_matches
                
                if total_count > 0:
                    matches_for_word.append({
                        'chunk': chunk,
                        'keyword': word,
                        'index': i,
                        'exact_count': exact_count,
                        'similar_count': similar_matches,
                        'total_count': total_count,
                        'article_info': f"ì œ{chunk.get('article_num', '?')}ì¡°({chunk.get('article_title', 'Unknown')})"
                    })
            
            # í•´ë‹¹ ë‹¨ì–´ì˜ ë§¤ì¹­ ê²°ê³¼ë¥¼ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
            matches_for_word.sort(key=lambda x: x['total_count'], reverse=True)
            
            # ìƒìœ„ 3ê°œë§Œ ì„ íƒ
            for match in matches_for_word[:3]:
                exact_matches.append(match)
                print(f"ğŸ¯ '{word}' ë°œê²¬: {match['article_info']} (ì •í™•:{match['exact_count']}, ìœ ì‚¬:{match['similar_count']})")
        
        # ì „ì²´ ë§¤ì¹­ ê²°ê³¼ë¥¼ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
        exact_matches.sort(key=lambda x: x['total_count'], reverse=True)
        
        return exact_matches[:5]
    
    def _perform_vector_search(self, query: str, top_k: int) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        print(f"ğŸ”¢ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰: '{query}'")
        
        vector_matches = []
        
        if self.vector_store.client and self.embedding_service.model:
            try:
                query_embedding = self.embedding_service.encode([query])
                results = self.vector_store.search(query_embedding[0], top_k=top_k)
                
                for result in results:
                    if result.score > 0.5:  # ì„ê³„ê°’
                        vector_matches.append({
                            'chunk': result.payload,
                            'score': result.score * 20,
                            'method': f'ë²¡í„°ê²€ìƒ‰({result.score:.3f})',
                            'index': result.id
                        })
                        print(f"ğŸ”¢ Qdrant ë§¤ì¹­: ì ìˆ˜ {result.score:.3f}")
            except Exception as e:
                print(f"âš ï¸ Qdrant ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ”¢ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(vector_matches)}ê°œ")
        return vector_matches
    
    def _create_enhanced_keyword_index(self, chunks: List[Dict], detected_contract_type: Optional[str] = None) -> Dict:
        """ê°•í™”ëœ í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ” ê°•í™”ëœ í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        keyword_index = {}
        
        # ê¸°ì¡´ ë²”ìš© í‚¤ì›Œë“œ
        universal_keywords = {
            'í•´ì§€_ì¢…ë£Œ': ['í•´ì§€', 'í•´ì œ', 'ì¢…ë£Œ', 'ì¤‘ë‹¨', 'íŒŒê¸°'],
            'ì˜ë¬´_ì±…ì„': ['ì˜ë¬´', 'ì±…ì„', 'ì´í–‰', 'ì¤€ìˆ˜'],
            'ê¶Œë¦¬_ìê²©': ['ê¶Œë¦¬', 'ê¶Œí•œ', 'ìê²©'],
            'ê³„ì•½_ì•½ì •': ['ê³„ì•½', 'í˜‘ì•½', 'ì•½ì •'],
            'ìœ„ë°˜_ì–´ê¹€': ['ìœ„ë°˜', 'ìœ„ë°°', 'ì–´ê¹€'],
            'ì†í•´_ë°°ìƒ': ['ì†í•´', 'ë°°ìƒ', 'í”¼í•´', 'ì†ì‹¤'],
            'ì§€ê¸‰_ê²°ì œ': ['ì§€ê¸‰', 'ì§€ë¶ˆ', 'ë‚©ë¶€', 'ê²°ì œ'],
            'ê¸°ê°„_ê¸°í•œ': ['ê¸°ê°„', 'ê¸°í•œ', 'ì¼ì', 'ë‚ ì§œ'],
            'ì‚¬ìœ _ì´ìœ ': ['ì‚¬ìœ ', 'ì´ìœ ', 'ì›ì¸', 'ê·¼ê±°'],
            'ìœ„í—˜_ì£¼ì˜': ['ìœ„í—˜', 'ì£¼ì˜', 'ê²½ê³ ', 'ìœ„í—˜ìš”ì†Œ']
        }
        
        # ê°ì§€ëœ ê³„ì•½ì„œ ìœ í˜•ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ê°€
        specialized_keywords = {}
        if detected_contract_type and detected_contract_type in CONTRACT_TYPES_TERMS:
            contract_terms = CONTRACT_TYPES_TERMS[detected_contract_type]
            
            for term in contract_terms:
                key = f"íŠ¹í™”_{term}"
                specialized_keywords[key] = [term]
                
                # ìœ ì‚¬ ìš©ì–´ ì¶”ê°€
                if 'ê³„ì•½' in term:
                    specialized_keywords[key].extend(['ê³„ì•½ì„œ', 'ì•½ì •ì„œ', 'í˜‘ì•½ì„œ'])
                if 'ê¸°ê°„' in term:
                    specialized_keywords[key].extend(['ê¸°í•œ', 'ì¼ì •', 'ë‚ ì§œ'])
                if 'ê¸ˆ' in term and term != 'ê¸ˆì§€':
                    specialized_keywords[key].extend(['ëŒ€ê¸ˆ', 'ë¹„ìš©', 'ìš”ê¸ˆ'])
            
            print(f"ğŸ“‹ {detected_contract_type} íŠ¹í™” í‚¤ì›Œë“œ {len(specialized_keywords)}ê°œ ì¶”ê°€")
        
        # ì „ì²´ í‚¤ì›Œë“œ ê²°í•©
        all_keywords = {**universal_keywords, **specialized_keywords}
        
        # ì¸ë±ì‹±
        for chunk_idx, chunk in enumerate(chunks):
            chunk_text = chunk['text'].lower()
            chunk_title = chunk['article_title'].lower()
            
            for keyword_group, keywords in all_keywords.items():
                score = 0
                found_keywords = []
                
                for keyword in keywords:
                    title_count = chunk_title.count(keyword)
                    text_count = chunk_text.count(keyword)
                    
                    if title_count > 0:
                        score += title_count * 15
                        found_keywords.append(f"{keyword}(ì œëª©)")
                    
                    if text_count > 0:
                        score += text_count * 3
                        found_keywords.append(f"{keyword}(ë³¸ë¬¸)")
                
                if score > 0:
                    if keyword_group not in keyword_index:
                        keyword_index[keyword_group] = []
                    
                    keyword_index[keyword_group].append({
                        'chunk_idx': chunk_idx,
                        'score': score,
                        'found_keywords': found_keywords,
                        'article_info': f"ì œ{chunk['article_num']}ì¡°({chunk['article_title']})" if chunk['article_num'] else chunk['article_title']
                    })
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        for keyword_group in keyword_index:
            keyword_index[keyword_group].sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… ì´ {len(keyword_index)}ê°œ í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±")
        return keyword_index
    
    def _create_vector_index(self, chunks: List[Dict]) -> bool:
        """ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            print(f"ğŸ”¢ {len(chunks)}ê°œ ì²­í¬ì— ëŒ€í•œ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ê°•í™”
            enhanced_texts = []
            payloads = []
            
            for chunk in chunks:
                title = chunk['article_title']
                text = chunk['text']
                enhanced_text = f"{title} {title} {title} {text}"
                enhanced_texts.append(enhanced_text)
                
                # Qdrantìš© í˜ì´ë¡œë“œ
                payload = {
                    'article_num': chunk['article_num'],
                    'article_title': chunk['article_title'],
                    'text': chunk['text'],
                    'type': chunk['type']
                }
                payloads.append(payload)
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self.embedding_service.encode(enhanced_texts, show_progress_bar=True)
            print(f"ğŸ“Š ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
            
            # Qdrantì— ì €ì¥
            if self.vector_store.client:
                if self.vector_store.create_collection(vector_size=embeddings.shape[1]):
                    if self.vector_store.add_vectors(embeddings, payloads):
                        print("âœ… Qdrant ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
                        return True
            
            print("âŒ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return False
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
        
    # rag/services/rag_engine.pyì— ì¶”ê°€í•  ë©”ì†Œë“œë“¤

    def validate_document_based_answer(self, answer: str) -> bool:
        """ë‹µë³€ì´ ë¬¸ì„œ ê¸°ë°˜ì¸ì§€ ê²€ì¦"""
        # ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì˜ íŠ¹ì§•
        document_indicators = [
            'ì œ', 'ì¡°', 'í•­', 'ê³„ì•½ì„œ', 'ì¡°í•­', 'ëª…ì‹œ', 'ê·œì •', 'ì •í•¨',
            'ë”°ë¥´ë©´', 'ì˜í•˜ë©´', 'ê¸°ì¬', 'í¬í•¨', 'ë‚´ìš©', 'ë¬¸ì„œ'
        ]
        
        # ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ì˜ íŠ¹ì§• (ê¸ˆì§€)
        general_indicators = [
            'ì¼ë°˜ì ìœ¼ë¡œ', 'ë³´í†µ', 'ëŒ€ê°œ', 'í†µìƒ', 'ë²•ë¥ ì—ì„œ', 'ë²•ì ìœ¼ë¡œ',
            'ì¼ë°˜ì ì¸', 'í†µìƒì ì¸', 'ë²•ë¥ ìƒ', 'ì¼ë°˜ë¡ '
        ]
        
        # ë¬¸ì„œ ê¸°ë°˜ ì§€í‘œ í™•ì¸
        doc_score = sum(1 for indicator in document_indicators if indicator in answer)
        
        # ì¼ë°˜ ì§€ì‹ ì§€í‘œ í™•ì¸ (íŒ¨ë„í‹°)
        general_score = sum(1 for indicator in general_indicators if indicator in answer)
        
        print(f"ğŸ” ë‹µë³€ ê²€ì¦: ë¬¸ì„œê¸°ë°˜({doc_score}) vs ì¼ë°˜ì§€ì‹({general_score})")
        
        # ë¬¸ì„œ ê¸°ë°˜ ì§€í‘œê°€ 2ê°œ ì´ìƒì´ê³ , ì¼ë°˜ ì§€ì‹ ì§€í‘œê°€ 1ê°œ ì´í•˜ì—¬ì•¼ í†µê³¼
        return doc_score >= 2 and general_score <= 1

    def generate_document_only_answer(self, question: str, context: str) -> str:
        """ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹µë³€ ìƒì„± (í´ë°±)"""
        from .translator import AnalysisService
        
        print("ğŸ”„ ë¬¸ì„œ ì „ìš© ë‹µë³€ ìƒì„± ì¤‘...")
        
        prompt = f"""ì•„ë˜ ê³„ì•½ì„œ ì¡°í•­ë“¤ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    ì§ˆë¬¸: {question}

    ê³„ì•½ì„œ ì¡°í•­ë“¤:
    {context}

    **ì ˆëŒ€ ê·œì¹™**:
    - ìœ„ ì¡°í•­ë“¤ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”
    - "ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ë²•ë¥ ìƒ" ê°™ì€ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
    - ë°˜ë“œì‹œ "ì œâ—‹ì¡°ì— ë”°ë¥´ë©´" í˜•ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”
    - ì¡°í•­ì— ì—†ìœ¼ë©´ "í•´ë‹¹ ë‚´ìš©ì´ ê³„ì•½ì„œì— ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”

    ì¡°í•­ì˜ ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

        try:
            analysis_service = AnalysisService()
            response = analysis_service.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ê³„ì•½ì„œ ì¡°í•­ë§Œì„ ì¸ìš©í•˜ëŠ” ì „ë¬¸ê°€. ì¼ë°˜ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.01
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            return f"ê³„ì•½ì„œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}"