# teamproject/legal_web/apps/documents/doc_retriever.py

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
import uuid

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import faiss
import fitz
import docx
from django.conf import settings
from qdrant_client import QdrantClient, models

# --- íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
def get_document_text(uploaded_file):
    print(f"ğŸ”„ get_document_text í•¨ìˆ˜ ì‹œì‘: {uploaded_file.name}")

    filename = uploaded_file.name
    ext = filename.split('.')[-1].lower()
    text = ""

    try:
        if ext == 'pdf':
            doc = fitz.open(stream=uploaded_file.read(), filetype='pdf')
            text = "\n".join(page.get_text() for page in doc)
        elif ext == 'docx':
            document = docx.Document(uploaded_file)
            text = "\n".join(p.text for p in document.paragraphs)
        elif ext == 'txt':
            text = uploaded_file.read().decode('utf-8')
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")
    except Exception as e:
        print(f"âŒ get_document_text í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    print(f"âœ… íŒŒì¼ '{filename}'ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)}ì)")
    print(f"ğŸ get_document_text í•¨ìˆ˜ ì¢…ë£Œ: {filename}")
    return text


# --- í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ë²¡í„°í™” ---

def recursive_split(text, separators, chunk_size):
    """ì¬ê·€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if len(text) <= chunk_size:
        return [text]
    
    # ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ êµ¬ë¶„ìë¶€í„° ì‹œë„
    current_separator = separators[0]
    next_separators = separators[1:]
    
    # í˜„ì¬ êµ¬ë¶„ìë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìœ¼ë©´, ë‹¤ìŒ êµ¬ë¶„ìë¡œ ì‹œë„
    if current_separator == "" or not next_separators:
        # ë” ì´ìƒ ë‚˜ëˆŒ êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´, ê¸€ì ìˆ˜ë¡œ ê°•ì œ ë¶„í• 
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    # êµ¬ë¶„ìë¡œ ë¶„í•  ì‹œë„
    try:
        parts = re.split(f'({current_separator})', text)
    except re.error:
        # ì •ê·œì‹ì´ ì•„ë‹Œ ì¼ë°˜ ë¬¸ìì—´ë¡œ ë¶„í• 
        parts = text.split(current_separator)

    chunks = []
    current_chunk = ""
    for part in parts:
        if len(current_chunk) + len(part) <= chunk_size:
            current_chunk += part
        else:
            # í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ê¸¸ë©´, ë” ì‘ì€ êµ¬ë¶„ìë¡œ ë‹¤ì‹œ ë‚˜ëˆ”
            if current_chunk:
                chunks.extend(recursive_split(current_chunk, next_separators, chunk_size))
            current_chunk = part
    if current_chunk:
        chunks.extend(recursive_split(current_chunk, next_separators, chunk_size))
        
    return chunks

def split_text_into_chunks_terms(text: str, chunk_size: int = 1500):
    """
    LangChainì˜ RecursiveCharacterTextSplitterì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ,
    ì—¬ëŸ¬ êµ¬ë¶„ìë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    if not text or not text.strip():
        return []

    print(f"ğŸ”„ [ìµœì¢… ì²­í‚¹ í•¨ìˆ˜] ì‹œì‘: chunk_size={chunk_size}")

    # êµ¬ë¶„ì ìš°ì„ ìˆœìœ„: ì¡°í•­ > ë¬¸ë‹¨ > ë¬¸ì¥ > ê³µë°±
    separators = [
        r'\nì œ\s*\d+\s*ì¡°',  # ì¡°í•­ (ê°€ì¥ í° ë‹¨ìœ„)
        '\n\n',            # ë¬¸ë‹¨
        '\n',              # ì¤„ë°”ê¿ˆ
        '. ',              # ë¬¸ì¥
        ' ',               # ë‹¨ì–´
        ''                 # ë§ˆì§€ë§‰ ê°•ì œ ë¶„í• 
    ]
    
    # 1. ì¬ê·€ì  ë¶„í•  ìˆ˜í–‰
    initial_chunks = recursive_split(text, separators, chunk_size)
    
    # 2. ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ê° ì²­í¬ê°€ ì–´ë–¤ ì¡°í•­ì— ì†í•˜ëŠ”ì§€ íŒŒì•…)
    final_chunks = []
    current_article_title = "ì„œë¬¸"
    article_pattern = r'(ì œ\s*\d+\s*ì¡°[^\n]*)'

    for chunk in initial_chunks:
        match = re.search(article_pattern, chunk)
        if match:
            # ì²­í¬ì—ì„œ ìƒˆë¡œìš´ ì¡°í•­ ì œëª©ì´ ë°œê²¬ë˜ë©´, í˜„ì¬ ì¡°í•­ ì œëª©ì„ ì—…ë°ì´íŠ¸
            current_article_title = match.group(1).strip()
        
        final_chunks.append(f"ì°¸ê³  ì¡°í•­: {current_article_title}\n\në‚´ìš©:\n{chunk.strip()}")
        
    print(f"ğŸ [ìµœì¢… ì²­í‚¹ í•¨ìˆ˜] ì¢…ë£Œ: {len(final_chunks)}ê°œ ì²­í¬ ìƒì„±")
    return final_chunks




import time 

def get_embeddings(client, texts: list[str]): 
    """
    í…ìŠ¤íŠ¸ ëª©ë¡ì„ ì—¬ëŸ¬ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ OpenAI ì„ë² ë”© APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    BATCH_SIZE = 100 
    
    all_embeddings = []
    
    print(f"ğŸ”„ get_embeddings í•¨ìˆ˜ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
    
    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i : i + BATCH_SIZE]
        batch_num = i//BATCH_SIZE + 1
        print(f"    - ë°°ì¹˜ #{batch_num} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ ì²­í¬)...")
        
        try:
            response = client.embeddings.create(
                input=batch,
                model="text-embedding-3-small"
            )           
            # ê²°ê³¼ ì €ì¥
            batch_embeddings = [np.array(embedding.embedding, dtype='float32') for embedding in response.data]
            all_embeddings.extend(batch_embeddings)

        except Exception as e:
            print(f"âŒ get_embeddings í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    print(f"ğŸ¤– OpenAI ì„ë² ë”© API í˜¸ì¶œ ì„±ê³µ: {len(all_embeddings)}ê°œ ë²¡í„° ìƒì„±")
    print(f"ğŸ get_embeddings í•¨ìˆ˜ ì¢…ë£Œ: ë²¡í„° ì°¨ì› {len(all_embeddings[0]) if all_embeddings else 0}")
    return all_embeddings


#  Qdrant ê´€ë ¨ í•¨ìˆ˜ (íšŒì›ìš©)
# ======================================================================

def get_qdrant_client():
    """Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print("ğŸ”„ get_qdrant_client í•¨ìˆ˜ ì‹œì‘")

    try:
        # settings.pyì— ì •ì˜ëœ ê°’ì„ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = QdrantClient(
            url=settings.QDRANT_URL,
            api_key=settings.QDRANT_API_KEY,
        )
        print("ğŸ”— Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")
        print("ğŸ get_qdrant_client í•¨ìˆ˜ ì¢…ë£Œ")
        return client
    except Exception as e:
        print(f"âŒ get_qdrant_client í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def upsert_vectors_to_qdrant(client: QdrantClient, chunks: list[str], vectors: list, user_id: int, session_id: str, payloads: list[dict] = None):
    """
    ë¬¸ì„œ ì¡°ê°ê³¼ ë²¡í„°ë¥¼ Qdrantì— ì €ì¥(upsert)í•©ë‹ˆë‹¤. (ìˆœìˆ˜ ì €ì¥ ë¡œì§ë§Œ ë‹´ë‹¹)
    
    Args:
        client: Qdrant í´ë¼ì´ì–¸íŠ¸
        chunks: í…ìŠ¤íŠ¸ ì¡°ê° ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ í˜ì´ë¡œë“œìš©)
        vectors: ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„° ë¦¬ìŠ¤íŠ¸
        user_id: ì‚¬ìš©ì ID
        session_id: ì„¸ì…˜ ID
        payloads: ì‚¬ì „ ì •ì˜ëœ í˜ì´ë¡œë“œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì , ê³„ì•½ì„œìš©)
    """
    print(f"ğŸ”„ upsert_vectors_to_qdrant í•¨ìˆ˜ ì‹œì‘: user_id={user_id}, session_id={session_id}, chunks={len(chunks)}ê°œ")

    if not chunks:
        print("âš ï¸ ì €ì¥í•  ì²­í¬ê°€ ì—†ì–´ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤")
        print("ğŸ upsert_vectors_to_qdrant í•¨ìˆ˜ ì¢…ë£Œ: ì²­í¬ ì—†ìŒ")
        return

    if len(chunks) != len(vectors):
        raise ValueError(f"ì²­í¬ ê°œìˆ˜({len(chunks)})ì™€ ë²¡í„° ê°œìˆ˜({len(vectors)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # payloadsê°€ ì œê³µëœ ê²½ìš° ê¸¸ì´ ê²€ì¦
    if payloads and len(chunks) != len(payloads):
        raise ValueError(f"ì²­í¬ ê°œìˆ˜({len(chunks)})ì™€ í˜ì´ë¡œë“œ ê°œìˆ˜({len(payloads)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    collection_name = "legal_documents"  # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì»¬ë ‰ì…˜ì— ì €ì¥

    # 1. ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
    try:
        client.get_collection(collection_name=collection_name)
        print(f"ğŸ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' í™•ì¸ ì™„ë£Œ")
    except Exception:  # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ ë°œìƒ
        print(f"ğŸ“ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=1536,  # text-embedding-3-smallì˜ ë²¡í„° ì°¨ì›
                distance=models.Distance.COSINE
            )
        )
        # ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        client.create_payload_index(collection_name=collection_name, field_name="user_id", field_schema="integer")
        client.create_payload_index(collection_name=collection_name, field_name="session_id", field_schema="keyword")

        # ê³„ì•½ì„œìš© ì¶”ê°€ ì¸ë±ìŠ¤ (ìˆì–´ë„ ì˜¤ë¥˜ ì•ˆë‚¨)
        try:
            client.create_payload_index(collection_name=collection_name, field_name="article_num", field_schema="integer")
            client.create_payload_index(collection_name=collection_name, field_name="type", field_schema="keyword")
        except:
            pass  # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ
                
            print(f"ğŸ“ ì»¬ë ‰ì…˜ '{collection_name}' ë° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    # 2. Qdrantì— ì €ì¥í•  í¬ì¸íŠ¸(Point) ìƒì„±
    print("ğŸ“¦ í¬ì¸íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    points = []

    for i, chunk in enumerate(chunks):
        # í˜ì´ë¡œë“œ ê²°ì •: ì‚¬ì „ ì •ì˜ëœ ê²ƒì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ìƒì„±
        if payloads:
            # ê³„ì•½ì„œ: ì‚¬ì „ ì •ì˜ëœ í˜ì´ë¡œë“œ + ê³µí†µ ë©”íƒ€ë°ì´í„°
            payload = {
                **payloads[i],  # ê¸°ì¡´ í˜ì´ë¡œë“œ (article_num, article_title, text, type ë“±)
                "user_id": user_id,
                "session_id": session_id
            }
        else:
            # ì•½ê´€: ê¸°ë³¸ í˜ì´ë¡œë“œ
            payload = {
                "text": chunk,
                "user_id": user_id,
                "session_id": session_id
            }

        points.append(
            models.PointStruct(
                id=str(uuid.uuid4()),  # ê° í¬ì¸íŠ¸ë§ˆë‹¤ ê³ ìœ  ID ìƒì„±
                vector=vectors[i].tolist(),  # ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°ë¥¼ list í˜•íƒœë¡œ ë³€í™˜
                payload=payload
            )
        )

    # 3. ë°ì´í„° ì—…ì„œíŠ¸(Upsert)
    if points:
        try: # ì „ì²´ upsert ì‹œë„ë¥¼ try ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
            print(f"ğŸ’¾ Qdrantì— {len(points)}ê°œ í¬ì¸íŠ¸ ì—…ì„œíŠ¸ ì¤‘...")
            client.upsert(collection_name=collection_name, points=points, wait=True)
            print(f"âœ… Qdrantì— {len(points)}ê°œì˜ í¬ì¸íŠ¸ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (user: {user_id}, session: {session_id})")
            print(f"ğŸ upsert_vectors_to_qdrant í•¨ìˆ˜ ì¢…ë£Œ: {len(points)}ê°œ í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ Qdrant ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ upsert_vectors_to_qdrant í•¨ìˆ˜ ì¢…ë£Œ: ì˜¤ë¥˜ ë°œìƒ")
            return False
    else: # pointsê°€ ë¹„ì–´ìˆì„ ê²½ìš° (chunksê°€ ë¹„ì–´ìˆìœ¼ë©´ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„)
        print("âš ï¸ ì €ì¥í•  í¬ì¸íŠ¸ê°€ ì—†ì–´ ì—…ì„œíŠ¸ ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("ğŸ upsert_vectors_to_qdrant í•¨ìˆ˜ ì¢…ë£Œ: ì €ì¥í•  í¬ì¸íŠ¸ ì—†ìŒ")
        return True # ì €ì¥ì´ í•„ìš” ì—†ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ê°„ì£¼

def search_qdrant(client: QdrantClient, embedding_client, query: str, user_id: int, session_id: str, top_k=5):
    """
    Qdrantì—ì„œ íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¹ì • ì„¸ì…˜ ë¬¸ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ search_qdrant í•¨ìˆ˜ ì‹œì‘: query='{query[:50]}...', user_id={user_id}, session_id={session_id}, top_k={top_k}")

    collection_name = "legal_documents"

    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    print("ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ ì¤‘...")
    query_vector = get_embeddings(embedding_client, [query])[0].tolist()

    # 2. í•„í„°(Filter) ìƒì„±: user_idì™€ session_idê°€ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰
    print("ğŸ¯ ê²€ìƒ‰ í•„í„° ìƒì„± ì¤‘...")
    query_filter = models.Filter(
        must=[
            models.FieldCondition(key="user_id", match=models.MatchValue(value=user_id)),
            models.FieldCondition(key="session_id", match=models.MatchValue(value=session_id)),
        ]
    )

    # 3. ê²€ìƒ‰ ìˆ˜í–‰
    print(f"ğŸ” Qdrant ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (ì»¬ë ‰ì…˜: {collection_name})")
    try:
        hits = client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            query_filter=query_filter,
            limit=top_k
        )

        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        results = [hit.payload['text'] for hit in hits]
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        print(f"ğŸ search_qdrant í•¨ìˆ˜ ì¢…ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
        return results
    except Exception as e:
        print(f"âŒ search_qdrant í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


# --- FAISS ì¸ë±ìŠ¤ ìƒì„± (ë¹„íšŒì›ìš©) ---
def create_faiss_index_from_vectors(vectors: list[np.ndarray]):
    """
    ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not vectors:
        print("âš ï¸ ë²¡í„°ê°€ ì—†ì–´ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None

    print(f"ğŸ”§ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ì°¨ì›: {len(vectors[0])})")
    vector_array = np.stack(vectors).astype(np.float32)
    dimension = vector_array.shape[1]

    index = faiss.IndexFlatL2(dimension)
    index.add(vector_array)  # type: ignore # pylint: disable=no-value-for-parameter
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„° ì¶”ê°€")
    return index

# --- FAISS ì¸ë±ìŠ¤ ê²€ìƒ‰ (ë¹„íšŒì›ìš©) ---
def search_faiss_index(index: faiss.Index, chunks: list[str], client, query: str, top_k=5):
    """
    ë©”ëª¨ë¦¬ì˜ FAISS ì¸ë±ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ search_faiss_index í•¨ìˆ˜ ì‹œì‘: query='{query[:50]}...', top_k={top_k}, ì´ {len(chunks)}ê°œ ì²­í¬")

    try:
        print("ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        query_embedding = get_embeddings(client, [query])[0]

        print("ğŸ” FAISS ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
        distances, indices = index.search(np.array([query_embedding]), top_k)

        results = [chunks[i] for i in indices[0]]
        print(f"âœ… FAISS ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        print(f"ğŸ“Š ê²€ìƒ‰ ê±°ë¦¬: {distances[0].tolist()}")
        print(f"ğŸ search_faiss_index í•¨ìˆ˜ ì¢…ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
        return results
    except Exception as e:
        print(f"âŒ search_faiss_index í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise



def get_all_chunks_from_qdrant(client: QdrantClient, user_id: int, session_id: str):
    """íŠ¹ì • ì‚¬ìš©ìì™€ ì„¸ì…˜ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì²­í¬ë¥¼ Qdrantì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print(f"ğŸ”„ Qdrantì—ì„œ ëª¨ë“  ì²­í¬ ë¡œë”© ì‹œì‘: user_id={user_id}, session_id={session_id}")
    try:
        # scroll APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í¬ì¸íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ë°ì´í„°ê°€ ë§¤ìš° í´ ê²½ìš° ì„±ëŠ¥ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        scrolled_points = client.scroll(
            collection_name="legal_documents",
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(key="user_id", match=models.MatchValue(value=user_id)),
                    models.FieldCondition(key="session_id", match=models.MatchValue(value=session_id)),
                ]
            ),
            limit=1000, # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜
            with_payload=True,
            with_vectors=False
        )
        all_chunks = [point.payload['text'] for point in scrolled_points[0]]
        print(f"âœ… Qdrantì—ì„œ {len(all_chunks)}ê°œ ì²­í¬ ë¡œë”© ì™„ë£Œ.")
        return all_chunks
    except Exception as e:
        print(f"âŒ Qdrantì—ì„œ ëª¨ë“  ì²­í¬ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []