# teamproject/legal_web/apps/documents/doc_retriever.py

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
import uuid

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import faiss
import fitz
import docx
from django.conf import settings
from qdrant_client import QdrantClient, models

# --- íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
def get_document_text(uploaded_file):
    print(f"ğŸ”„ get_document_text í•¨ìˆ˜ ì‹œì‘: {uploaded_file.name}")

    filename = uploaded_file.name
    ext = filename.split('.')[-1].lower()
    text = ""

    try:
        if ext == 'pdf':
            doc = fitz.open(stream=uploaded_file.read(), filetype='pdf')
            text = "\n".join(page.get_text() for page in doc)
        elif ext == 'docx':
            document = docx.Document(uploaded_file)
            text = "\n".join(p.text for p in document.paragraphs)
        elif ext == 'txt':
            text = uploaded_file.read().decode('utf-8')
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")
    except Exception as e:
        print(f"âŒ get_document_text í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise ValueError(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    print(f"âœ… íŒŒì¼ '{filename}'ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)}ì)")
    print(f"ğŸ get_document_text í•¨ìˆ˜ ì¢…ë£Œ: {filename}")
    return text


# --- í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ë²¡í„°í™” ---

def split_text_into_chunks_terms(text: str, chunk_size: int = 1500):
    """
    1. 'ì œNì¡°'ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤.
    2. ê° ì¡°í•­ì´ ë„ˆë¬´ ê¸¸ë©´ chunk_sizeì— ë§ì¶° ë‹¤ì‹œ ìë¦…ë‹ˆë‹¤.
    3. ëª¨ë“  ìµœì¢… ì²­í¬ì— ì¶œì²˜(ì¡°í•­ ì œëª©)ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    if not text:
        return []

    print(f"--- 'split_text_into_chunks_terms' í•¨ìˆ˜ ì‹¤í–‰ ì‹œì‘ (ìµœëŒ€ ì²­í¬ í¬ê¸°: {chunk_size}ì) ---")

    # 'ì œNì¡°' íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œë¥¼ (ì œëª©, ë‚´ìš©) ìŒìœ¼ë¡œ ë¶„ë¦¬
    pattern = r'(ì œ\s*\d+\s*ì¡°[^\n]*)'
    split_parts = re.split(pattern, text)

    articles = []
    # ì²« ë¶€ë¶„(ì¡°í•­ ì‹œì‘ ì „)ì´ ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´ 'ì„œë¬¸' ë“±ìœ¼ë¡œ ì²˜ë¦¬
    if split_parts[0].strip():
        articles.append(("ì„œë¬¸", split_parts[0].strip()))

    for i in range(1, len(split_parts), 2):
        article_title = split_parts[i].strip()
        article_content = split_parts[i+1].strip() if (i + 1) < len(split_parts) else ""
        articles.append((article_title, article_content))

    if not articles and text:
        articles = [("ë¬¸ì„œ ì „ì²´", text)]

    print(f"  - 'ì œNì¡°' íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ {len(articles)}ê°œì˜ ì¡°í•­/ë¶€ë¶„ìœ¼ë¡œ 1ì°¨ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # ê° ì¡°í•­ì„ ì¬ë¶„í• í•˜ë©°, ëª¨ë“  ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    final_chunks = []
    for article_title, article_content in articles:
        if not article_content.strip():
            continue

        if len(article_content) > chunk_size:
            print(f"  - ì •ë³´: ê¸´ ì¡°í•­ '{article_title}' (ê¸¸ì´: {len(article_content)})ì„/ë¥¼ ì¬ë¶„í• í•©ë‹ˆë‹¤.")
            for i in range(0, len(article_content), chunk_size):
                sub_chunk_content = article_content[i : i + chunk_size]
                final_chunks.append(f"ì°¸ê³  ì¡°í•­: {article_title}\n\në‚´ìš©:\n{sub_chunk_content}")
        else:
            final_chunks.append(f"ì°¸ê³  ì¡°í•­: {article_title}\n\në‚´ìš©:\n{article_content}")

    final_chunk_list = [chunk for chunk in final_chunks if chunk.strip()]
    print(f"--- 'split_text_into_chunks' í•¨ìˆ˜ ì¢…ë£Œ. ìµœì¢… ë°˜í™˜ ì²­í¬ ê°œìˆ˜: {len(final_chunk_list)}ê°œ ---")

    return final_chunk_list

def get_embeddings(client, texts: list[str]):
    # ... (ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì •ì ì¸ ë²„ì „)
    BATCH_SIZE = 100
    all_embeddings = []
    for i in range(0, len(texts), BATCH_SIZE):
        batch = texts[i : i + BATCH_SIZE]
        response = client.embeddings.create(input=batch, model="text-embedding-3-small")
        all_embeddings.extend([np.array(e.embedding, dtype='float32') for e in response.data])
    return all_embeddings

#  Qdrant ê´€ë ¨ í•¨ìˆ˜ (íšŒì›ìš©)
# ======================================================================

def get_qdrant_client():
    """Qdrant í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print("ğŸ”„ get_qdrant_client í•¨ìˆ˜ ì‹œì‘")

    try:
        # settings.pyì— ì •ì˜ëœ ê°’ì„ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = QdrantClient(
            url=settings.QDRANT_URL,
            api_key=settings.QDRANT_API_KEY,
        )
        print("ğŸ”— Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")
        print("ğŸ get_qdrant_client í•¨ìˆ˜ ì¢…ë£Œ")
        return client
    except Exception as e:
        print(f"âŒ get_qdrant_client í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def upsert_vectors_to_qdrant(client: QdrantClient, chunks: list[str], vectors: list, user_id: int, session_id: str):
    """
    ë¬¸ì„œ ì¡°ê°ê³¼ ë²¡í„°ë¥¼ Qdrantì— ì €ì¥(upsert)í•©ë‹ˆë‹¤. (ìˆœìˆ˜ ì €ì¥ ë¡œì§ë§Œ ë‹´ë‹¹)
    
    Args:
        client: Qdrant í´ë¼ì´ì–¸íŠ¸
        chunks: í…ìŠ¤íŠ¸ ì¡°ê° ë¦¬ìŠ¤íŠ¸
        vectors: ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„° ë¦¬ìŠ¤íŠ¸
        user_id: ì‚¬ìš©ì ID
        session_id: ì„¸ì…˜ ID
    """
    print(f"ğŸ”„ upsert_vectors_to_qdrant í•¨ìˆ˜ ì‹œì‘: user_id={user_id}, session_id={session_id}, chunks={len(chunks)}ê°œ")

    if not chunks:
        print("âš ï¸ ì €ì¥í•  ì²­í¬ê°€ ì—†ì–´ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤")
        print("ğŸ upsert_vectors_to_qdrant í•¨ìˆ˜ ì¢…ë£Œ: ì²­í¬ ì—†ìŒ")
        return

    if len(chunks) != len(vectors):
        raise ValueError(f"ì²­í¬ ê°œìˆ˜({len(chunks)})ì™€ ë²¡í„° ê°œìˆ˜({len(vectors)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    collection_name = "legal_documents"  # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì»¬ë ‰ì…˜ì— ì €ì¥

    # 1. ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
    try:
        client.get_collection(collection_name=collection_name)
        print(f"ğŸ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' í™•ì¸ ì™„ë£Œ")
    except Exception:  # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ ë°œìƒ
        print(f"ğŸ“ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(
                size=1536,  # text-embedding-3-smallì˜ ë²¡í„° ì°¨ì›
                distance=models.Distance.COSINE
            )
        )
        # ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        client.create_payload_index(collection_name=collection_name, field_name="user_id", field_schema="integer")
        client.create_payload_index(collection_name=collection_name, field_name="session_id", field_schema="keyword")
        print(f"ğŸ“ ì»¬ë ‰ì…˜ '{collection_name}' ë° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    # 2. Qdrantì— ì €ì¥í•  í¬ì¸íŠ¸(Point) ìƒì„±
    print("ğŸ“¦ í¬ì¸íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    points = []
    for i, chunk in enumerate(chunks):
        points.append(
            models.PointStruct(
                id=str(uuid.uuid4()),  # ê° í¬ì¸íŠ¸ë§ˆë‹¤ ê³ ìœ  ID ìƒì„±
                vector=vectors[i].tolist(),  # ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°ë¥¼ list í˜•íƒœë¡œ ë³€í™˜
                payload={
                    "text": chunk,
                    "user_id": user_id,
                    "session_id": session_id
                }
            )
        )

    # 3. ë°ì´í„° ì—…ì„œíŠ¸(Upsert)
    if points:
        print(f"ğŸ’¾ Qdrantì— {len(points)}ê°œ í¬ì¸íŠ¸ ì—…ì„œíŠ¸ ì¤‘...")
        client.upsert(collection_name=collection_name, points=points, wait=True)
        print(f"âœ… Qdrantì— {len(points)}ê°œì˜ í¬ì¸íŠ¸ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. (user: {user_id}, session: {session_id})")

    print(f"ğŸ upsert_vectors_to_qdrant í•¨ìˆ˜ ì¢…ë£Œ: {len(points)}ê°œ í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ")

def search_qdrant(client: QdrantClient, embedding_client, query: str, user_id: int, session_id: str, top_k=5):
    """
    Qdrantì—ì„œ íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¹ì • ì„¸ì…˜ ë¬¸ì„œë¥¼ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ search_qdrant í•¨ìˆ˜ ì‹œì‘: query='{query[:50]}...', user_id={user_id}, session_id={session_id}, top_k={top_k}")

    collection_name = "legal_documents"

    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    print("ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ ì¤‘...")
    query_vector = get_embeddings(embedding_client, [query])[0].tolist()

    # 2. í•„í„°(Filter) ìƒì„±: user_idì™€ session_idê°€ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰
    print("ğŸ¯ ê²€ìƒ‰ í•„í„° ìƒì„± ì¤‘...")
    query_filter = models.Filter(
        must=[
            models.FieldCondition(key="user_id", match=models.MatchValue(value=user_id)),
            models.FieldCondition(key="session_id", match=models.MatchValue(value=session_id)),
        ]
    )

    # 3. ê²€ìƒ‰ ìˆ˜í–‰
    print(f"ğŸ” Qdrant ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (ì»¬ë ‰ì…˜: {collection_name})")
    try:
        hits = client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            query_filter=query_filter,
            limit=top_k
        )

        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        results = [hit.payload['text'] for hit in hits]
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        print(f"ğŸ search_qdrant í•¨ìˆ˜ ì¢…ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
        return results
    except Exception as e:
        print(f"âŒ search_qdrant í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


# --- FAISS ì¸ë±ìŠ¤ ìƒì„± (ë¹„íšŒì›ìš©) ---
def create_faiss_index_from_vectors(vectors: list[np.ndarray]):
    """
    ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not vectors:
        print("âš ï¸ ë²¡í„°ê°€ ì—†ì–´ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None

    print(f"ğŸ”§ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ì°¨ì›: {len(vectors[0])})")
    vector_array = np.stack(vectors).astype(np.float32)
    dimension = vector_array.shape[1]

    index = faiss.IndexFlatL2(dimension)
    index.add(vector_array)  # type: ignore # pylint: disable=no-value-for-parameter
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index.ntotal}ê°œ ë²¡í„° ì¶”ê°€")
    return index

# --- FAISS ì¸ë±ìŠ¤ ê²€ìƒ‰ (ë¹„íšŒì›ìš©) ---
def search_faiss_index(index: faiss.Index, chunks: list[str], client, query: str, top_k=5):
    """
    ë©”ëª¨ë¦¬ì˜ FAISS ì¸ë±ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ search_faiss_index í•¨ìˆ˜ ì‹œì‘: query='{query[:50]}...', top_k={top_k}, ì´ {len(chunks)}ê°œ ì²­í¬")

    try:
        print("ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        query_embedding = get_embeddings(client, [query])[0]

        print("ğŸ” FAISS ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
        distances, indices = index.search(np.array([query_embedding]), top_k)

        results = [chunks[i] for i in indices[0]]
        print(f"âœ… FAISS ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        print(f"ğŸ“Š ê²€ìƒ‰ ê±°ë¦¬: {distances[0].tolist()}")
        print(f"ğŸ search_faiss_index í•¨ìˆ˜ ì¢…ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
        return results
    except Exception as e:
        print(f"âŒ search_faiss_index í•¨ìˆ˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
