# teamproject/legal_web/apps/documents/doc_services.py

from openai import OpenAI, APIError
from django.conf import settings
from django.contrib.auth.models import AnonymousUser

from . import doc_retriever
from . import doc_prompt_manager
from . import doc_retriever_content

import traceback

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (settings.pyì— OPENAI_API_KEY ì„¤ì •)
client = OpenAI(api_key=settings.OPENAI_API_KEY)
qdrant_client = doc_retriever.get_qdrant_client()

import re



def _translate_text(text: str, language: str) -> str:
    """
    doc_prompt_managerì—ì„œ ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ê°€ì ¸ì™€ ë²ˆì—­í•˜ê³ ,
    í›„ì²˜ë¦¬ë¡œ ë°˜ë³µë˜ëŠ” ê´„í˜¸ ì„¤ëª…ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not text or language == 'ko':
        return text

    # 1. doc_prompt_managerì—ì„œ 'en' ê°™ì€ í‚¤ë¡œ 1ì°¨ ë”•ì…”ë„ˆë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    prompt_template_dict = doc_prompt_manager.TRANSLATION_PROMPTS.get(language)
    
    if prompt_template_dict and "translation_prompt_template" in prompt_template_dict:
        prompt = prompt_template_dict["translation_prompt_template"].format(text=text)
    else:
        # í…œí”Œë¦¿ì„ ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        print(f"  - Warning: '{language}'ì— ëŒ€í•œ ì „ìš© ë²ˆì—­ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì§€ ëª»í•´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        lang_name_map = {'en': 'English', 'ja': 'Japanese', 'zh': 'Chinese', 'es': 'Spanish'}
        target_lang_name = lang_name_map.get(language, language)
        prompt = f"Translate the following Korean text to {target_lang_name}:\n\n{text}"

    try:
        # 4. API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
        )
        translated_text = response.choices[0].message.content.strip()

        # 5. í›„ì²˜ë¦¬ ë¡œì§
        print("  - ë²ˆì—­ ì™„ë£Œ. í›„ì²˜ë¦¬ ì‹œì‘...")

        # â˜…â˜…â˜…â˜…â˜… ì—¬ê¸°ê°€ í•µì‹¬ í›„ì²˜ë¦¬ ë¡œì§ì…ë‹ˆë‹¤ â˜…â˜…â˜…â˜…â˜…
        # í›„ì²˜ë¦¬ ëŒ€ìƒ ìš©ì–´ì™€ ê·¸ íŒ¨í„´ ì •ì˜
        terms_to_clean = {
            'contract': r'(contracts?)\s*\(.*?\)',
            'damages': r'(damages?)\s*\(.*?\)',
            # í•„ìš”í•œ ë‹¤ë¥¸ ìš©ì–´ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        }

        cleaned_text = translated_text
        for term, pattern in terms_to_clean.items():
            # 1. ê´„í˜¸ ì„¤ëª…ì„ í¬í•¨í•œ ëª¨ë“  ë§¤ì¹­ ê²°ê³¼ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            matches = list(re.finditer(pattern, cleaned_text, re.IGNORECASE))

            # 2. ë§Œì•½ 2ê°œ ì´ìƒ ë°œê²¬ë˜ì—ˆë‹¤ë©´ (ì¦‰, ë°˜ë³µì´ ìˆë‹¤ë©´)
            if len(matches) > 1:
                # 3. ì²« ë²ˆì§¸ ë§¤ì¹­ ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
                first_match_end_pos = matches[0].end()
                first_part = cleaned_text[:first_match_end_pos]
                
                # 4. ì²« ë²ˆì§¸ ë§¤ì¹­ ì´í›„ì˜ í…ìŠ¤íŠ¸ì—ì„œë§Œ, íŒ¨í„´ì„ ì°¾ì•„ ê´„í˜¸ ì—†ëŠ” ë‹¨ì–´ë¡œ êµì²´í•©ë‹ˆë‹¤.
                remaining_part = cleaned_text[first_match_end_pos:]
                
                # re.subì˜ ì„¸ ë²ˆì§¸ ì¸ì count=... ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ëª¨ë“  ë§¤ì¹­ì„ êµì²´
                cleaned_remaining_part = re.sub(pattern, r'\1', remaining_part, flags=re.IGNORECASE)

                # 5. ë‘ ë¶€ë¶„ì„ ë‹¤ì‹œ í•©ì¹©ë‹ˆë‹¤.
                cleaned_text = first_part + cleaned_remaining_part
                print(f"    - '{term}'ì— ëŒ€í•œ ë°˜ë³µ ì„¤ëª… {len(matches) - 1}ê°œë¥¼ ì„±ê³µì ìœ¼ë¡œ ì œê±°í–ˆìŠµë‹ˆë‹¤.")

        return cleaned_text.strip()

    except Exception as e:
        print(f"Translation Error for {language}: {e}")
        return f"(Translation to {language} failed) {text}"


    


# ì•½ê´€ 
def analyze_terms_document(user, uploaded_file, session_id, language='ko', doc_type='terms'):
    """
    ë¬¸ì„œ ë¶„ì„ì˜ ê° ë‹¨ê³„ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥í•˜ë©° ì‹¤í–‰í•˜ê³ , ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì¬ê·€ì  ìš”ì•½ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì•ˆì •ì ì¸ ë²„ì „ì…ë‹ˆë‹¤.
    """
    try:
        print(f"[ì•½ê´€ ë¶„ì„] ì‹œì‘: {uploaded_file.name}")

        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            raise ValueError("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        doc_type_name = "ì´ìš©ì•½ê´€"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")

        # --- 2. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ ---
        try:
            # --- 2-1. Map-Reduce ìš”ì•½ ---
            summary_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=4000)
            individual_summaries = []
            for i, chunk in enumerate(summary_chunks):
                summary_prompt = doc_prompt_manager.get_summarize_chunk_terms_prompt(chunk, doc_type_name)
                response = client.chat.completions.create(
                    model="gpt-4o-mini", messages=[{"role": "user", "content": summary_prompt}],
                    max_tokens=300, temperature=0.3
                )
                individual_summaries.append(response.choices[0].message.content)
            print(f"  - [Map ë‹¨ê³„ ì™„ë£Œ] ìƒì„±í•œ ê°œë³„ ìš”ì•½ë³¸ : {len(individual_summaries)}ê°œ")

            # --- 2-2. ì¬ê·€ì  ìš”ì•½ (Reduce ë‹¨ê³„) ---
            current_summaries = individual_summaries
            while len(current_summaries) > 1:
                print(f"    - í˜„ì¬ ìš”ì•½ë³¸ {len(current_summaries)}ê°œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ë‹¤ì‹œ ìš”ì•½í•©ë‹ˆë‹¤...")
                next_level_summaries = []
                # 10ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
                for i in range(0, len(current_summaries), 10):
                    batch = current_summaries[i:i+10]
                    reduce_prompt = doc_prompt_manager.get_combine_summaries_terms_prompt(batch, doc_type_name)
                    intermediate_summary = client.chat.completions.create(
                        model="gpt-4o-mini", messages=[{"role": "user", "content": reduce_prompt}],
                        max_tokens=1500, temperature=0.5
                    ).choices[0].message.content
                    next_level_summaries.append(intermediate_summary)
                current_summaries = next_level_summaries

            final_summary_ko = current_summaries[0] if current_summaries else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            print("  - [Reduce ë‹¨ê³„ ì™„ë£Œ] ìµœì¢… ìš”ì•½ë³¸ì„ ìƒì„±")

            # --- 2-3. ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ---
            risk_text_ko_prompt = doc_prompt_manager.get_risk_factors_terms_prompt(document_text)
            risk_text_ko = client.chat.completions.create(
                model="gpt-4o-mini", messages=[{"role": "user", "content": risk_text_ko_prompt}],
                max_tokens=1000, temperature=0.3
            ).choices[0].message.content
            print("  - [2ë‹¨ê³„ ì™„ë£Œ] ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì™„ë£Œ")

        except Exception as summary_e:
            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜ - 2ë‹¨ê³„] ìš”ì•½ ë˜ëŠ” ìœ„í—˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {summary_e}")
            traceback.print_exc() 
            raise summary_e

        # --- 3. ë²ˆì—­ ---
        print(f"\n[3ë‹¨ê³„] ê²°ê³¼ë¥¼ '{language}' ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤...")
        korean_full_analysis = f"## ğŸ“‹ Key Analysis of Terms and Conditions\n\n{final_summary_ko}\n\n---\n\n## âš ï¸ Major Risk Factors and Precautions\n\n{risk_text_ko}"

        # â˜…â˜…â˜…â˜…â˜… _translate_text í•¨ìˆ˜ì— 'language' ë³€ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤ â˜…â˜…â˜…â˜…â˜…
        final_analysis_lang = _translate_text(korean_full_analysis, language)
        
        print("[3ë‹¨ê³„ ì™„ë£Œ] ë²ˆì—­ ì„±ê³µ.")


        # 4. QAë¥¼ ìœ„í•œ ë²¡í„°í™”
        qa_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=1500)
        chunk_count = len(qa_chunks)
        print(f"  - í…ìŠ¤íŠ¸ê°€ {chunk_count}ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

        print("  - í…ìŠ¤íŠ¸ ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘...")
        vectors = doc_retriever.get_embeddings(client, qa_chunks)
        print(f"  - ì´ {len(vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ.")
        
        if isinstance(user, AnonymousUser):
            print("  - FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = doc_retriever.create_faiss_index_from_vectors(vectors)

            storage_data = {"type": "faiss", "index": faiss_index, "chunks": qa_chunks}
            print("  - ë¹„íšŒì›ìš© FAISS ì¸ë±ìŠ¤ ë° ì²­í¬ ì €ì¥ ì™„ë£Œ.")
        else:
            doc_retriever.upsert_vectors_to_qdrant(qdrant_client, qa_chunks, vectors, user.id, session_id)
            storage_data = {"type": "qdrant"}

                    

        # 5. ê²°ê³¼ ë°˜í™˜
        
        print(f"[ì•½ê´€ ë¶„ì„] ì™„ë£Œ: {uploaded_file.name}")
        return {
            "success": True,
            "summary": final_analysis_lang,
            "storage_data": storage_data,
            "message": "ì•½ê´€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except APIError as e:
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {e.status_code})" # pylint: disable=no-member
        if e.code == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        print(f"[ERROR] ì•½ê´€ ë¶„ì„ - OpenAI API Error: {e}")
        return {"success": False, "error": error_message}

    except Exception as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {"success": False, "error": f"ì„œë²„ ë‚´ë¶€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "status_code": 500}

# ----------------------------------------------------------

def analyze_contract_document(user, uploaded_file, session_id, language='ko'):
    print("\n[í•¨ìˆ˜ ì‹œì‘] 'analyze_contract_document'ê°€ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    try:
        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        print("[1ë‹¨ê³„] íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            print("[1ë‹¨ê³„ ì˜¤ë¥˜] ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "error": "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status_code": 400}
        doc_type_name = "ê³„ì•½ì„œ"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")


        # --- 2. ê³„ì•½ì„œ ìœ í˜• ê°ì§€ ë° ì¡°í•­ë³„ ì¶”ì¶œ ---
        print("[2ë‹¨ê³„] ê³„ì•½ì„œ ìœ í˜• ê°ì§€ ë° ì¡°í•­ ì¶”ì¶œ ì‹œì‘...")
        detected_contract_type, confidence, contract_type_info = doc_retriever_content.detect_contract_type(document_text)
        if confidence and isinstance(confidence, dict):
            confidence_percentage = confidence.get('percentage', 0)
            print(f"  - ê°ì§€ëœ ê³„ì•½ì„œ ìœ í˜•: {detected_contract_type} (ì‹ ë¢°ë„: {confidence_percentage}%)")
        else:
            print(f"  - ê°ì§€ëœ ê³„ì•½ì„œ ìœ í˜•: {detected_contract_type}")

        document_chunks_raw = doc_retriever_content.extract_articles_with_content(document_text)
        if not document_chunks_raw:
             print("[2ë‹¨ê³„ ì˜¤ë¥˜] í…ìŠ¤íŠ¸ë¥¼ ìœ íš¨í•œ ì¡°í•­ ì²­í¬ë¡œ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return {"success": False, "error": "ê³„ì•½ì„œ ì¡°í•­ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 400}
        chunk_count = len(document_chunks_raw)
        print(f"ğŸ“‹ ì´ {chunk_count}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.")

        # --- 3. í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ---
        print("[3ë‹¨ê³„] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        keyword_index = doc_retriever_content.create_enhanced_keyword_index(document_chunks_raw, detected_contract_type)
        print(f"[3ë‹¨ê³„ ì™„ë£Œ] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ.")

        # --- 4. í…ìŠ¤íŠ¸ ê°•í™” ë° ë²¡í„°í™”(ì„ë² ë”©) ---
        print("[4ë‹¨ê³„] í…ìŠ¤íŠ¸ ê°•í™” ë° ì„ë² ë”© ìƒì„± ì‹œì‘...")
        enhanced_texts, payloads = doc_retriever_content.enhance_document_texts(
            document_chunks_raw, # ì›ë³¸ ì²­í¬ ì‚¬ìš©
            user_id=user.id if user.is_authenticated else None, # AnonymousUserì¼ ê²½ìš° None ì „ë‹¬
            session_id=session_id
        )
        if not enhanced_texts or not payloads:
            print("[4ë‹¨ê³„ ì˜¤ë¥˜] í…ìŠ¤íŠ¸ ê°•í™” ë˜ëŠ” í˜ì´ë¡œë“œ ìƒì„± ì‹¤íŒ¨.")
            return {"success": False, "error": "ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°•í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 500}

        print(" Â - í…ìŠ¤íŠ¸ ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘...")
        # ì—¬ê¸°ì„œ ì •ì˜ëœ openai_clientë¥¼ ì‚¬ìš©
        vectors = doc_retriever.get_embeddings(client, enhanced_texts)
        if not vectors:
            print("[4ë‹¨ê³„ ì˜¤ë¥˜] ë²¡í„° ìƒì„± ì‹¤íŒ¨.")
            return {"success": False, "error": "í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 500}
        print(f"[4ë‹¨ê³„ ì™„ë£Œ] ì´ {len(vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ.")

        # --- 5. ë²¡í„° ì €ì¥ (FAISS ë˜ëŠ” Qdrant) ---
        storage_data = {} # ê²°ê³¼ì— í¬í•¨ë  ìŠ¤í† ë¦¬ì§€ ì •ë³´
        print("[5ë‹¨ê³„] ë²¡í„° ì €ì¥ ì²˜ë¦¬ (íšŒì›/ë¹„íšŒì› êµ¬ë¶„)...")
        if isinstance(user, AnonymousUser):
            print(" Â - ë¹„íšŒì›: FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = doc_retriever.create_faiss_index_from_vectors(vectors)
            if faiss_index is None:
                print(" Â - FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨.")
                return {"success": False, "error": "FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨.", "status_code": 500}
            storage_data = {"type": "faiss", "index": faiss_index, "chunks": enhanced_texts, "payloads": payloads}
            print(" Â - ë¹„íšŒì›ìš© FAISS ì¸ë±ìŠ¤ ë° ì²­í¬ ì €ì¥ ì™„ë£Œ.")
        else:
            print(" Â - íšŒì›: Qdrant DBì— ë²¡í„° ì €ì¥ ì‹œì‘...")
            # qdrant_clientê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if qdrant_client is None:
                print(" Â - Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íšŒì›ì¸ë° DB ì—°ê²° ì‹¤íŒ¨).")
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ (Qdrant í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ).", "status_code": 500}

            upsert_success = doc_retriever.upsert_vectors_to_qdrant(
                client=qdrant_client,
                chunks=enhanced_texts,
                vectors=vectors,
                user_id=user.id,
                session_id=session_id,
                payloads=payloads  # ê³„ì•½ì„œìš© ìƒì„¸ í˜ì´ë¡œë“œ ì¶”ê°€
            )
            if not upsert_success:
                print(" Â - Qdrant DBì— ë²¡í„° ì €ì¥ ì‹¤íŒ¨.")
                return {"success": False, "error": "Qdrantì— ë²¡í„° ì €ì¥ ì‹¤íŒ¨.", "status_code": 500}
            storage_data = {"type": "qdrant"}
            print(f" Â - íšŒì›(ID:{user.id})ìš© Qdrant DBì— ì €ì¥ ì™„ë£Œ.")


        # --- 6. í†µí•© ë¶„ì„ (ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„) ---
        print(f"[6ë‹¨ê³„] {language} í†µì¼ëœ ë¶„ì„ ì‹œì‘ (í•œêµ­ì–´ ê¸°ì¤€ ë²ˆì—­ ë°©ì‹)...")
        analysis_result_tuple = doc_retriever_content.unified_analysis_with_translation(client, document_text, language)
        if not isinstance(analysis_result_tuple, tuple) or len(analysis_result_tuple) != 2:
            print(f"[6ë‹¨ê³„ ì˜¤ë¥˜] unified_analysis_with_translationì´ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì˜ ê°’ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {analysis_result_tuple}")
            return {"success": False,
                    "error": "í†µí•© ë¶„ì„ ì„œë¹„ìŠ¤ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•ì‹.",
                    "status_code": 500}

        analysis_result_summary = analysis_result_tuple[0] # íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìš”ì•½
        analysis_result_risk = analysis_result_tuple[1]    # íŠœí”Œì˜ ë‘ ë²ˆì§¸ ìš”ì†Œê°€ ìœ„í—˜ ë¶„ì„

        # ì´ì „ì— ì •ì˜ëœ chunk_count ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ê³„ì‚°í•©ë‹ˆë‹¤.
        # ì˜ˆ: chunk_count = len(document_chunks_raw)
        chunk_count = len(document_chunks_raw) if 'document_chunks_raw' in locals() else 0 # ë˜ëŠ” enhanced_texts ë“±

        print("[6ë‹¨ê³„ ì™„ë£Œ] í†µí•© ë¶„ì„ ì™„ë£Œ. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„ë¨.")

        print("ìµœì¢… ë‹µë³€")
        final_combined_summary = doc_retriever_content.format_contract_analysis_result(
            detected_contract_type, confidence, analysis_result_summary, analysis_result_risk, "í•œêµ­ì–´", chunk_count
        )


        # --- ìµœì¢… ì„±ê³µ ë°˜í™˜ (ìš”ì²­í•˜ì‹  í˜•ì‹) ---
        print("\n[í•¨ìˆ˜ ì¢…ë£Œ] 'analyze_contract_document' ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return {
            "success": True,
            "summary": final_combined_summary,
            "storage_data": storage_data, # ê¸°ì¡´ storage_data ë³€ìˆ˜ ì‚¬ìš©
            "chunk_count": chunk_count # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œ
        }

    # â˜…â˜…â˜… ë°”ê¹¥ìª½ ìµœì¢… ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ â˜…â˜…â˜…
    except APIError as e:
        print("\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] OpenAI API ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")
        status_code = getattr(e, 'status_code', 500)
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {status_code})"

        if getattr(e, 'code', None) == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        return {
            "success": False,
            "error": error_message,
            "status_code": status_code
        }

    except Exception as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤: {str(e)}")

        # ì–¸ì–´ë³„ ì˜¤ë¥˜ ë©”ì‹œì§€
        error_messages = {
            "í•œêµ­ì–´": f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "æ—¥æœ¬èª": f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "ä¸­æ–‡": f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
            "English": f"âŒ Error occurred during processing: {str(e)}",
            "EspaÃ±ol": f"âŒ Error ocurrido durante el procesamiento: {str(e)}"
        }
        
        localized_error = error_messages.get(language, error_messages["í•œêµ­ì–´"])
        
        return {
            "success": False, 
            "error": localized_error, 
            "status_code": 500
        }
