# teamproject/legal_web/apps/documents/doc_services.py

from openai import OpenAI, APIError
from django.conf import settings
from django.contrib.auth.models import AnonymousUser

from . import doc_retriever
from . import doc_prompt_manager
from . import doc_retriever_content

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (settings.pyì— OPENAI_API_KEY ì„¤ì •)
client = OpenAI(api_key=settings.OPENAI_API_KEY)
qdrant_client = doc_retriever.get_qdrant_client()

def _translate_text(text: str, target_lang_name: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.
    """
    if not text or target_lang_name == "í•œêµ­ì–´":
        return text
    try:
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_lang_name}ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ì„œì‹(ì¤„ë°”ê¿ˆ, ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ë“±)ì„ ìµœëŒ€í•œ ìœ ì§€í•´ì£¼ì„¸ìš”:\n\n---\n{text}"
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2, # ë²ˆì—­ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •
        )
        return response.choices[0].message.content.strip()
    except APIError as e:
        print(f"OpenAI API Error during translation: {e}")
        # API ì˜¤ë¥˜ ë°œìƒ ì‹œ, ë²ˆì—­ ì‹¤íŒ¨ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì›ë¬¸ ë°˜í™˜
        return f"({target_lang_name} ë²ˆì—­ ì‹¤íŒ¨: API ì˜¤ë¥˜) {text}"
    except Exception as e:
        print(f"Unexpected error during translation: {e}")
        return f"({target_lang_name} ë²ˆì—­ ì‹¤íŒ¨: ì‹œìŠ¤í…œ ì˜¤ë¥˜) {text}"

# ì•½ê´€
def analyze_terms_document(user, uploaded_file, session_id, language='ko'):
    """
    ë¬¸ì„œ ë¶„ì„ì˜ ê° ë‹¨ê³„ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥í•˜ë©° ì‹¤í–‰í•˜ê³ , ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì¬ê·€ì  ìš”ì•½ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì•ˆì •ì ì¸ ë²„ì „ì…ë‹ˆë‹¤.
    """
    try:
        print("\n[í•¨ìˆ˜ ì‹œì‘] 'analyze_terms_document'ê°€ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        print("[1ë‹¨ê³„] íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            raise ValueError("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        doc_type_name = "ì´ìš©ì•½ê´€"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")

        # --- 2. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ ---
        # ì´ ì „ì²´ ë¸”ë¡ì„ ë³„ë„ì˜ try-exceptë¡œ ê°ì‹¸ì„œ ë¬¸ì œ ì§€ì ì„ íŠ¹ì •í•©ë‹ˆë‹¤.
        try:
            print("\n[2ë‹¨ê³„] AIë¥¼ ì´ìš©í•œ ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

            # --- 2-1. Map-Reduce ìš”ì•½ ---
            print("  - [2-1 ì‹œì‘] ë¬¸ì„œë¥¼ ìš”ì•½ìš© ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
            summary_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=4000)
            print(f"  - ìš”ì•½ì„ ìœ„í•´ ë¬¸ì„œë¥¼ {len(summary_chunks)}ê°œì˜ ì²­í¬ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤.")

            print("  - [Map ë‹¨ê³„] ê° ì²­í¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤...")
            individual_summaries = []
            for i, chunk in enumerate(summary_chunks):
                summary_prompt = doc_prompt_manager.get_summarize_chunk_terms_prompt(chunk, doc_type_name)
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo", messages=[{"role": "user", "content": summary_prompt}],
                    max_tokens=300, temperature=0.3
                )
                individual_summaries.append(response.choices[0].message.content)
            print(f"  - [Map ë‹¨ê³„ ì™„ë£Œ] {len(individual_summaries)}ê°œì˜ ê°œë³„ ìš”ì•½ë³¸ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

            # --- 2-2. ì¬ê·€ì  ìš”ì•½ (Reduce ë‹¨ê³„) ---
            print("  - [Reduce ë‹¨ê³„] ìš”ì•½ë³¸ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            current_summaries = individual_summaries
            while len(current_summaries) > 1:
                print(f"    - í˜„ì¬ ìš”ì•½ë³¸ {len(current_summaries)}ê°œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ë‹¤ì‹œ ìš”ì•½í•©ë‹ˆë‹¤...")
                next_level_summaries = []
                # 10ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
                for i in range(0, len(current_summaries), 10):
                    batch = current_summaries[i:i+10]
                    reduce_prompt = doc_prompt_manager.get_combine_summaries_terms_prompt(batch, doc_type_name)
                    intermediate_summary = client.chat.completions.create(
                        model="gpt-3.5-turbo", messages=[{"role": "user", "content": reduce_prompt}],
                        max_tokens=1500, temperature=0.5
                    ).choices[0].message.content
                    next_level_summaries.append(intermediate_summary)
                current_summaries = next_level_summaries

            final_summary_ko = current_summaries[0] if current_summaries else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            print("  - [Reduce ë‹¨ê³„ ì™„ë£Œ] ìµœì¢… ìš”ì•½ë³¸ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

            # --- 2-3. ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ---
            print("  - [ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì‹œì‘]...")
            risk_text_ko_prompt = doc_prompt_manager.get_risk_factors_terms_prompt(document_text)
            risk_text_ko = client.chat.completions.create(
                model="gpt-3.5-turbo", messages=[{"role": "user", "content": risk_text_ko_prompt}],
                max_tokens=1000, temperature=0.3
            ).choices[0].message.content
            print("  - [ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì™„ë£Œ].")
            print("[2ë‹¨ê³„ ì™„ë£Œ] ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ì´ ëª¨ë‘ ëë‚¬ìŠµë‹ˆë‹¤.")

        except Exception as summary_e:
            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜ - 2ë‹¨ê³„] ìš”ì•½ ë˜ëŠ” ìœ„í—˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {summary_e}", exc_info=True)
            raise summary_e

        # --- 3. ë²ˆì—­ ---
        print(f"\n[3ë‹¨ê³„] ê²°ê³¼ë¥¼ '{language}' ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤...")
        final_summary_lang, risk_text_lang = final_summary_ko, risk_text_ko
        lang_map = {'en': 'English', 'es': 'Spanish', 'ja': 'ì¼ë³¸ì–´', 'zh': 'ì¤‘êµ­ì–´'}
        if language in lang_map:
            target_lang_name = lang_map[language]
            final_summary_lang = _translate_text(final_summary_ko, target_lang_name)
            risk_text_lang = _translate_text(risk_text_ko, target_lang_name)
        print("[3ë‹¨ê³„ ì™„ë£Œ] ë²ˆì—­ ì„±ê³µ.")

        # --- 4. QAë¥¼ ìœ„í•œ ë²¡í„°í™” ---
        try:
            print("\n[4ë‹¨ê³„] ì§ˆì˜ì‘ë‹µ(QA)ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            qa_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=1500)
            chunk_count = len(qa_chunks)
            print(f"  - í…ìŠ¤íŠ¸ê°€ {chunk_count}ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

            print("  - í…ìŠ¤íŠ¸ ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘...")
            vectors = doc_retriever.get_embeddings(client, qa_chunks)
            print(f"  - ì´ {len(vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ.")

            if isinstance(user, AnonymousUser):
                print("  - FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
                faiss_index = doc_retriever.create_faiss_index_from_vectors(vectors)

                storage_data = {"type": "faiss", "index": faiss_index, "chunks": qa_chunks}
                print("  - ë¹„íšŒì›ìš© FAISS ì¸ë±ìŠ¤ ë° ì²­í¬ ì €ì¥ ì™„ë£Œ.")
            else:
                print("  - Qdrant DBì— ë²¡í„° ì €ì¥ ì‹œì‘...")

                doc_retriever.upsert_vectors_to_qdrant(
                    client=qdrant_client,
                    chunks=qa_chunks,
                    vectors=vectors,
                    user_id=user.id,
                    session_id=session_id
                )
                storage_data = {"type": "qdrant"}
                print(f"  - íšŒì›(ID:{user.id})ìš© Qdrant DBì— ì €ì¥ ì™„ë£Œ.")

            print("[4ë‹¨ê³„ ì™„ë£Œ] ë²¡í„°í™” ë° ì €ì¥ ì„±ê³µ.")
        except Exception as vector_e:
            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜ - 4ë‹¨ê³„] ë²¡í„°í™” ë˜ëŠ” DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {vector_e}", exc_info=True)
            raise vector_e

        # --- 5. ìµœì¢… ê²°ê³¼ ë°˜í™˜ ---
        print("\n[ì„±ê³µ] ëª¨ë“  ë‹¨ê³„ê°€ ì™„ë£Œë˜ì–´ ì„±ê³µ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return {
            "success": True,
            "summary": f"ğŸ“‹ ë¬¸ì„œ ìš”ì•½\n\n{final_summary_lang}\n\n---\n\nâš ï¸ ìœ„í—˜ ìš”ì†Œ ì‹ë³„\n\n{risk_text_lang}",
            "storage_data": storage_data,
            "chunk_count": chunk_count
        }

    # â˜…â˜…â˜… ë°”ê¹¥ìª½ ìµœì¢… ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ â˜…â˜…â˜…
    except APIError as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] OpenAI API ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")
        status_code = getattr(e, 'status_code', 500)
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {status_code})"

        if getattr(e, 'code', None) == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        return {
            "success": False,
            "error": error_message,
            "status_code": status_code
        }

    except Exception as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")
        return {"success": False, "error": "ì„œë²„ ë‚´ë¶€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "status_code": 500}

# ----------------------------------------------------------

def analyze_contract_document(user, uploaded_file, session_id, language='ko'):
    print("\n[í•¨ìˆ˜ ì‹œì‘] 'analyze_contract_document'ê°€ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    try:
        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        print("[1ë‹¨ê³„] íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            print("[1ë‹¨ê³„ ì˜¤ë¥˜] ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "error": "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status_code": 400}
        doc_type_name = "ê³„ì•½ì„œ"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")


        # --- 2. ê³„ì•½ì„œ ìœ í˜• ê°ì§€ ë° ì¡°í•­ë³„ ì¶”ì¶œ ---
        print("[2ë‹¨ê³„] ê³„ì•½ì„œ ìœ í˜• ê°ì§€ ë° ì¡°í•­ ì¶”ì¶œ ì‹œì‘...")
        detected_contract_type, confidence, contract_type_info = doc_retriever_content.detect_contract_type(document_text)
        print(f" Â - ê°ì§€ëœ ê³„ì•½ì„œ ìœ í˜•: {detected_contract_type} (ì‹ ë¢°ë„: {confidence:.2f})")

        document_chunks_raw = doc_retriever_content.extract_articles_with_content(document_text)
        if not document_chunks_raw:
             print("[2ë‹¨ê³„ ì˜¤ë¥˜] í…ìŠ¤íŠ¸ë¥¼ ìœ íš¨í•œ ì¡°í•­ ì²­í¬ë¡œ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return {"success": False, "error": "ê³„ì•½ì„œ ì¡°í•­ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 400}
        print(f"ğŸ“‹ ì´ {len(document_chunks_raw)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.")
        chunk_count = len(document_chunks_raw)

        # --- 3. í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ---
        print("[3ë‹¨ê³„] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        keyword_index = doc_retriever_content.create_enhanced_keyword_index(document_chunks_raw, detected_contract_type)
        print(f"[3ë‹¨ê³„ ì™„ë£Œ] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ.")

        # --- 4. í…ìŠ¤íŠ¸ ê°•í™” ë° ë²¡í„°í™”(ì„ë² ë”©) ---
        print("[4ë‹¨ê³„] í…ìŠ¤íŠ¸ ê°•í™” ë° ì„ë² ë”© ìƒì„± ì‹œì‘...")
        enhanced_texts, payloads = doc_retriever_content.enhance_document_texts(
            document_chunks_raw, # ì›ë³¸ ì²­í¬ ì‚¬ìš©
            user_id=user.id if user.is_authenticated else None, # AnonymousUserì¼ ê²½ìš° None ì „ë‹¬
            session_id=session_id
        )
        if not enhanced_texts or not payloads:
            print("[4ë‹¨ê³„ ì˜¤ë¥˜] í…ìŠ¤íŠ¸ ê°•í™” ë˜ëŠ” í˜ì´ë¡œë“œ ìƒì„± ì‹¤íŒ¨.")
            return {"success": False, "error": "ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°•í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 500}

        print(" Â - í…ìŠ¤íŠ¸ ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘...")
        # ì—¬ê¸°ì„œ ì •ì˜ëœ openai_clientë¥¼ ì‚¬ìš©
        vectors = doc_retriever.get_embeddings(client, enhanced_texts)
        if not vectors:
            print("[4ë‹¨ê³„ ì˜¤ë¥˜] ë²¡í„° ìƒì„± ì‹¤íŒ¨.")
            return {"success": False, "error": "í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 500}
        print(f"[4ë‹¨ê³„ ì™„ë£Œ] ì´ {len(vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ.")

        # --- 5. ë²¡í„° ì €ì¥ (FAISS ë˜ëŠ” Qdrant) ---
        storage_data = {} # ê²°ê³¼ì— í¬í•¨ë  ìŠ¤í† ë¦¬ì§€ ì •ë³´
        print("[5ë‹¨ê³„] ë²¡í„° ì €ì¥ ì²˜ë¦¬ (íšŒì›/ë¹„íšŒì› êµ¬ë¶„)...")
        if isinstance(user, AnonymousUser):
            print(" Â - ë¹„íšŒì›: FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = doc_retriever.create_faiss_index_from_vectors(vectors)
            if faiss_index is None:
                print(" Â - FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨.")
                return {"success": False, "error": "FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨.", "status_code": 500}
            storage_data = {"type": "faiss", "index": faiss_index, "chunks": enhanced_texts, "payloads": payloads}
            print(" Â - ë¹„íšŒì›ìš© FAISS ì¸ë±ìŠ¤ ë° ì²­í¬ ì €ì¥ ì™„ë£Œ.")
        else:
            print(" Â - íšŒì›: Qdrant DBì— ë²¡í„° ì €ì¥ ì‹œì‘...")
            # qdrant_clientê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if qdrant_client is None:
                print(" Â - Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íšŒì›ì¸ë° DB ì—°ê²° ì‹¤íŒ¨).")
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ (Qdrant í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ).", "status_code": 500}

            upsert_success = doc_retriever.upsert_vectors_to_qdrant(
                client=qdrant_client,
                chunks=enhanced_texts,
                vectors=vectors,
                user_id=user.id,
                session_id=session_id,
                payloads=payloads  # ê³„ì•½ì„œìš© ìƒì„¸ í˜ì´ë¡œë“œ ì¶”ê°€
            )
            if not upsert_success:
                print(" Â - Qdrant DBì— ë²¡í„° ì €ì¥ ì‹¤íŒ¨.")
                return {"success": False, "error": "Qdrantì— ë²¡í„° ì €ì¥ ì‹¤íŒ¨.", "status_code": 500}
            storage_data = {"type": "qdrant"}
            print(f" Â - íšŒì›(ID:{user.id})ìš© Qdrant DBì— ì €ì¥ ì™„ë£Œ.")


        # --- 6. í†µí•© ë¶„ì„ (ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„) ---
        print(f"[6ë‹¨ê³„] {language} í†µì¼ëœ ë¶„ì„ ì‹œì‘ (í•œêµ­ì–´ ê¸°ì¤€ ë²ˆì—­ ë°©ì‹)...")
        
        # doc_retriever_content.unified_analysis_with_translation í•¨ìˆ˜ í˜¸ì¶œ
        # ì´ í•¨ìˆ˜ê°€ (final_summary_lang_string, risk_text_lang_string) í˜•íƒœì˜ íŠœí”Œì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
        analysis_result_tuple = doc_retriever_content.unified_analysis_with_translation(client, document_text, language) # client ì¸ì ì—†ëŠ” ê¸°ì¡´ í•¨ìˆ˜ì— ë§ì¶° ìˆ˜ì •

        # !!!!!!! ì—¬ê¸°ì„œ íŠœí”Œì„ ì§ì ‘ ì–¸íŒ¨í‚¹í•˜ì—¬ ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤ !!!!!!!
        # ì´ì œ analysis_result_tuple.get('success', False) ì´ëŸ° ì½”ë“œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ì˜¤ë¥˜ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤ë©´, analysis_result_tupleì˜ ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ë•Œë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
        if not isinstance(analysis_result_tuple, tuple) or len(analysis_result_tuple) != 2:
            print(f"[6ë‹¨ê³„ ì˜¤ë¥˜] unified_analysis_with_translationì´ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì˜ ê°’ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {analysis_result_tuple}")
            return {"success": False,
                    "error": "í†µí•© ë¶„ì„ ì„œë¹„ìŠ¤ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•ì‹.",
                    "status_code": 500}

        final_summary_lang = analysis_result_tuple[0] # íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìš”ì•½
        risk_text_lang = analysis_result_tuple[1]    # íŠœí”Œì˜ ë‘ ë²ˆì§¸ ìš”ì†Œê°€ ìœ„í—˜ ë¶„ì„

        # ì´ì „ì— ì •ì˜ëœ chunk_count ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ê³„ì‚°í•©ë‹ˆë‹¤.
        # ì˜ˆ: chunk_count = len(document_chunks_raw)
        chunk_count = len(document_chunks_raw) if 'document_chunks_raw' in locals() else 0 # ë˜ëŠ” enhanced_texts ë“±

        print("[6ë‹¨ê³„ ì™„ë£Œ] í†µí•© ë¶„ì„ ì™„ë£Œ. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„ë¨.")

        # --- ìµœì¢… ì„±ê³µ ë°˜í™˜ (ìš”ì²­í•˜ì‹  í˜•ì‹) ---
        print("\n[í•¨ìˆ˜ ì¢…ë£Œ] 'analyze_contract_document' ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return {
            "success": True,
            "summary": f"ğŸ“‹ ë¬¸ì„œ ìš”ì•½\n\n{final_summary_lang}\n\n---\n\nâš ï¸ ìœ„í—˜ ìš”ì†Œ ì‹ë³„\n\n{risk_text_lang}",
            "storage_data": storage_data, # ê¸°ì¡´ storage_data ë³€ìˆ˜ ì‚¬ìš©
            "chunk_count": chunk_count # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œ
        }

        #return {
        #    "success": True,
        #    "summary": f"ğŸ“‹ ë¬¸ì„œ ìš”ì•½\n\n{summary}\n\n---\n\nâš ï¸ ì ì¬ì  ìœ„í—˜ ìš”ì†Œ ì‹ë³„\n\n{risk_analysis}\n\n---\n\n ìœ í˜• : {detected_contract_type}, ì •ë³´ : {contract_type_info}",
        #    "storage_data": storage_data, # FAISS ë˜ëŠ” Qdrant ì €ì¥ ì •ë³´ í¬í•¨
        #    "chunk_count": chunk_count
        #}

    # â˜…â˜…â˜… ë°”ê¹¥ìª½ ìµœì¢… ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ â˜…â˜…â˜…
    except APIError as e:
        print("\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] OpenAI API ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")
        status_code = getattr(e, 'status_code', 500)
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {status_code})"

        if getattr(e, 'code', None) == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        return {
            "success": False,
            "error": error_message,
            "status_code": status_code
        }

    except Exception as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤: {e}")
        return {"success": False, "error": f"ì„œë²„ ë‚´ë¶€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", "status_code": 500}
