# teamproject/legal_web/apps/documents/doc_services.py

from openai import OpenAI, APIError
from django.conf import settings
from django.contrib.auth.models import AnonymousUser

from . import doc_retriever
from . import doc_prompt_manager
from . import doc_retriever_content

import traceback

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (settings.pyì— OPENAI_API_KEY ì„¤ì •)
client = OpenAI(api_key=settings.OPENAI_API_KEY)
qdrant_client = doc_retriever.get_qdrant_client()

def _translate_text(text: str, target_lang_name: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.
    """
    if not text or target_lang_name == "í•œêµ­ì–´":
        return text
    try:
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_lang_name}ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ì„œì‹(ì¤„ë°”ê¿ˆ, ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ë“±)ì„ ìµœëŒ€í•œ ìœ ì§€í•´ì£¼ì„¸ìš”:\n\n---\n{text}"
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2, # ë²ˆì—­ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •
        )
        return response.choices[0].message.content.strip()
    except APIError as e:
        print(f"OpenAI API Error during translation: {e}")
        # API ì˜¤ë¥˜ ë°œìƒ ì‹œ, ë²ˆì—­ ì‹¤íŒ¨ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì›ë¬¸ ë°˜í™˜
        return f"({target_lang_name} ë²ˆì—­ ì‹¤íŒ¨: API ì˜¤ë¥˜) {text}"
    except Exception as e:
        print(f"Unexpected error during translation: {e}")
        return f"({target_lang_name} ë²ˆì—­ ì‹¤íŒ¨: ì‹œìŠ¤í…œ ì˜¤ë¥˜) {text}"

# ì•½ê´€ 
def analyze_terms_document(user, uploaded_file, session_id, language='ko', doc_type='terms'):
    """
    ë¬¸ì„œ ë¶„ì„ì˜ ê° ë‹¨ê³„ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥í•˜ë©° ì‹¤í–‰í•˜ê³ , ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì¬ê·€ì  ìš”ì•½ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì•ˆì •ì ì¸ ë²„ì „ì…ë‹ˆë‹¤.
    """
    try:
        print(f"[ì•½ê´€ ë¶„ì„] ì‹œì‘: {uploaded_file.name}")

        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            raise ValueError("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        doc_type_name = "ì´ìš©ì•½ê´€"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")

        # --- 2. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ ---
        try:
            # --- 2-1. Map-Reduce ìš”ì•½ ---
            summary_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=4000)
            individual_summaries = []
            for i, chunk in enumerate(summary_chunks):
                summary_prompt = doc_prompt_manager.get_summarize_chunk_terms_prompt(chunk, doc_type_name)
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo", messages=[{"role": "user", "content": summary_prompt}],
                    max_tokens=300, temperature=0.3
                )
                individual_summaries.append(response.choices[0].message.content)
            print(f"  - [Map ë‹¨ê³„ ì™„ë£Œ] ìƒì„±í•œ ê°œë³„ ìš”ì•½ë³¸ : {len(individual_summaries)}ê°œ")

            # --- 2-2. ì¬ê·€ì  ìš”ì•½ (Reduce ë‹¨ê³„) ---
            current_summaries = individual_summaries
            while len(current_summaries) > 1:
                print(f"    - í˜„ì¬ ìš”ì•½ë³¸ {len(current_summaries)}ê°œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ë‹¤ì‹œ ìš”ì•½í•©ë‹ˆë‹¤...")
                next_level_summaries = []
                # 10ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
                for i in range(0, len(current_summaries), 10):
                    batch = current_summaries[i:i+10]
                    reduce_prompt = doc_prompt_manager.get_combine_summaries_terms_prompt(batch, doc_type_name)
                    intermediate_summary = client.chat.completions.create(
                        model="gpt-3.5-turbo", messages=[{"role": "user", "content": reduce_prompt}],
                        max_tokens=1500, temperature=0.5
                    ).choices[0].message.content
                    next_level_summaries.append(intermediate_summary)
                current_summaries = next_level_summaries

            final_summary_ko = current_summaries[0] if current_summaries else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            print("  - [Reduce ë‹¨ê³„ ì™„ë£Œ] ìµœì¢… ìš”ì•½ë³¸ì„ ìƒì„±")

            # --- 2-3. ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ---
            risk_text_ko_prompt = doc_prompt_manager.get_risk_factors_terms_prompt(document_text)
            risk_text_ko = client.chat.completions.create(
                model="gpt-3.5-turbo", messages=[{"role": "user", "content": risk_text_ko_prompt}],
                max_tokens=1000, temperature=0.3
            ).choices[0].message.content
            print("  - [2ë‹¨ê³„ ì™„ë£Œ] ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì™„ë£Œ")

        except Exception as summary_e:
            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜ - 2ë‹¨ê³„] ìš”ì•½ ë˜ëŠ” ìœ„í—˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {summary_e}")
            traceback.print_exc() 
            raise summary_e

        # --- 3. ë²ˆì—­ ---
        print(f"\n[3ë‹¨ê³„] ê²°ê³¼ë¥¼ '{language}' ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤...")
        
        korean_full_analysis = f"## ğŸ“‹ ì•½ê´€ í•µì‹¬ ë¶„ì„\n\n{final_summary_ko}\n\n---\n\n## âš ï¸ ì£¼ìš” ìœ„í—˜ ìš”ì†Œ ë° ìœ ì˜ì‚¬í•­\n\n{risk_text_ko}"

        lang_map = {'en': 'English', 'es': 'Spanish', 'ja': 'ì¼ë³¸ì–´', 'zh': 'ì¤‘êµ­ì–´'}
        target_lang_name = lang_map.get(language, "í•œêµ­ì–´")
        
        final_analysis_lang = _translate_text(korean_full_analysis, target_lang_name)
        print("[3ë‹¨ê³„ ì™„ë£Œ] ë²ˆì—­ ì„±ê³µ.")


        # 4. QAë¥¼ ìœ„í•œ ë²¡í„°í™”
        qa_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=1500)
        chunk_count = len(qa_chunks)
        print(f"  - í…ìŠ¤íŠ¸ê°€ {chunk_count}ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

        print("  - í…ìŠ¤íŠ¸ ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘...")
        vectors = doc_retriever.get_embeddings(client, qa_chunks)
        print(f"  - ì´ {len(vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ.")
        
        if isinstance(user, AnonymousUser):
            print("  - FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = doc_retriever.create_faiss_index_from_vectors(vectors)

            storage_data = {"type": "faiss", "index": faiss_index, "chunks": qa_chunks}
            print("  - ë¹„íšŒì›ìš© FAISS ì¸ë±ìŠ¤ ë° ì²­í¬ ì €ì¥ ì™„ë£Œ.")
        else:
            doc_retriever.upsert_vectors_to_qdrant(qdrant_client, qa_chunks, vectors, user.id, session_id)
            storage_data = {"type": "qdrant"}

                    

        # 5. ê²°ê³¼ ë°˜í™˜
        
        print(f"[ì•½ê´€ ë¶„ì„] ì™„ë£Œ: {uploaded_file.name}")
        return {
            "success": True,
            "summary": final_analysis_lang,
            "storage_data": storage_data,
            "message": "ì•½ê´€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except APIError as e:
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {e.status_code})" # pylint: disable=no-member
        if e.code == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        print(f"[ERROR] ì•½ê´€ ë¶„ì„ - OpenAI API Error: {e}")
        return {"success": False, "error": error_message}

    except Exception as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {"success": False, "error": f"ì„œë²„ ë‚´ë¶€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "status_code": 500}

# ----------------------------------------------------------

def analyze_contract_document(user, uploaded_file, session_id, language='ko'):
    print("\n[í•¨ìˆ˜ ì‹œì‘] 'analyze_contract_document'ê°€ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    try:
        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        print("[1ë‹¨ê³„] íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            print("[1ë‹¨ê³„ ì˜¤ë¥˜] ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "error": "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status_code": 400}
        doc_type_name = "ê³„ì•½ì„œ"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")


        # --- 2. ê³„ì•½ì„œ ìœ í˜• ê°ì§€ ë° ì¡°í•­ë³„ ì¶”ì¶œ ---
        print("[2ë‹¨ê³„] ê³„ì•½ì„œ ìœ í˜• ê°ì§€ ë° ì¡°í•­ ì¶”ì¶œ ì‹œì‘...")
        detected_contract_type, confidence, contract_type_info = doc_retriever_content.detect_contract_type(document_text)
        if confidence and isinstance(confidence, dict):
            confidence_percentage = confidence.get('percentage', 0)
            print(f"  - ê°ì§€ëœ ê³„ì•½ì„œ ìœ í˜•: {detected_contract_type} (ì‹ ë¢°ë„: {confidence_percentage}%)")
        else:
            print(f"  - ê°ì§€ëœ ê³„ì•½ì„œ ìœ í˜•: {detected_contract_type}")

        document_chunks_raw = doc_retriever_content.extract_articles_with_content(document_text)
        if not document_chunks_raw:
             print("[2ë‹¨ê³„ ì˜¤ë¥˜] í…ìŠ¤íŠ¸ë¥¼ ìœ íš¨í•œ ì¡°í•­ ì²­í¬ë¡œ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return {"success": False, "error": "ê³„ì•½ì„œ ì¡°í•­ ë¶„í• ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 400}
        chunk_count = len(document_chunks_raw)
        print(f"ğŸ“‹ ì´ {chunk_count}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.")

        # --- 3. í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ---
        print("[3ë‹¨ê³„] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        keyword_index = doc_retriever_content.create_enhanced_keyword_index(document_chunks_raw, detected_contract_type)
        print(f"[3ë‹¨ê³„ ì™„ë£Œ] í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ.")

        # --- 4. í…ìŠ¤íŠ¸ ê°•í™” ë° ë²¡í„°í™”(ì„ë² ë”©) ---
        print("[4ë‹¨ê³„] í…ìŠ¤íŠ¸ ê°•í™” ë° ì„ë² ë”© ìƒì„± ì‹œì‘...")
        enhanced_texts, payloads = doc_retriever_content.enhance_document_texts(
            document_chunks_raw, # ì›ë³¸ ì²­í¬ ì‚¬ìš©
            user_id=user.id if user.is_authenticated else None, # AnonymousUserì¼ ê²½ìš° None ì „ë‹¬
            session_id=session_id
        )
        if not enhanced_texts or not payloads:
            print("[4ë‹¨ê³„ ì˜¤ë¥˜] í…ìŠ¤íŠ¸ ê°•í™” ë˜ëŠ” í˜ì´ë¡œë“œ ìƒì„± ì‹¤íŒ¨.")
            return {"success": False, "error": "ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°•í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 500}

        print(" Â - í…ìŠ¤íŠ¸ ë²¡í„°í™”(ì„ë² ë”©) ì‹œì‘...")
        # ì—¬ê¸°ì„œ ì •ì˜ëœ openai_clientë¥¼ ì‚¬ìš©
        vectors = doc_retriever.get_embeddings(client, enhanced_texts)
        if not vectors:
            print("[4ë‹¨ê³„ ì˜¤ë¥˜] ë²¡í„° ìƒì„± ì‹¤íŒ¨.")
            return {"success": False, "error": "í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "status_code": 500}
        print(f"[4ë‹¨ê³„ ì™„ë£Œ] ì´ {len(vectors)}ê°œ ë²¡í„° ìƒì„± ì™„ë£Œ.")

        # --- 5. ë²¡í„° ì €ì¥ (FAISS ë˜ëŠ” Qdrant) ---
        storage_data = {} # ê²°ê³¼ì— í¬í•¨ë  ìŠ¤í† ë¦¬ì§€ ì •ë³´
        print("[5ë‹¨ê³„] ë²¡í„° ì €ì¥ ì²˜ë¦¬ (íšŒì›/ë¹„íšŒì› êµ¬ë¶„)...")
        if isinstance(user, AnonymousUser):
            print(" Â - ë¹„íšŒì›: FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = doc_retriever.create_faiss_index_from_vectors(vectors)
            if faiss_index is None:
                print(" Â - FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨.")
                return {"success": False, "error": "FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨.", "status_code": 500}
            storage_data = {"type": "faiss", "index": faiss_index, "chunks": enhanced_texts, "payloads": payloads}
            print(" Â - ë¹„íšŒì›ìš© FAISS ì¸ë±ìŠ¤ ë° ì²­í¬ ì €ì¥ ì™„ë£Œ.")
        else:
            print(" Â - íšŒì›: Qdrant DBì— ë²¡í„° ì €ì¥ ì‹œì‘...")
            # qdrant_clientê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if qdrant_client is None:
                print(" Â - Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íšŒì›ì¸ë° DB ì—°ê²° ì‹¤íŒ¨).")
                return {"success": False, "error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ (Qdrant í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ).", "status_code": 500}

            upsert_success = doc_retriever.upsert_vectors_to_qdrant(
                client=qdrant_client,
                chunks=enhanced_texts,
                vectors=vectors,
                user_id=user.id,
                session_id=session_id,
                payloads=payloads  # ê³„ì•½ì„œìš© ìƒì„¸ í˜ì´ë¡œë“œ ì¶”ê°€
            )
            if not upsert_success:
                print(" Â - Qdrant DBì— ë²¡í„° ì €ì¥ ì‹¤íŒ¨.")
                return {"success": False, "error": "Qdrantì— ë²¡í„° ì €ì¥ ì‹¤íŒ¨.", "status_code": 500}
            storage_data = {"type": "qdrant"}
            print(f" Â - íšŒì›(ID:{user.id})ìš© Qdrant DBì— ì €ì¥ ì™„ë£Œ.")


        # --- 6. í†µí•© ë¶„ì„ (ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„) ---
        print(f"[6ë‹¨ê³„] {language} í†µì¼ëœ ë¶„ì„ ì‹œì‘ (í•œêµ­ì–´ ê¸°ì¤€ ë²ˆì—­ ë°©ì‹)...")
        analysis_result_tuple = doc_retriever_content.unified_analysis_with_translation(client, document_text, language)
        if not isinstance(analysis_result_tuple, tuple) or len(analysis_result_tuple) != 2:
            print(f"[6ë‹¨ê³„ ì˜¤ë¥˜] unified_analysis_with_translationì´ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì˜ ê°’ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {analysis_result_tuple}")
            return {"success": False,
                    "error": "í†µí•© ë¶„ì„ ì„œë¹„ìŠ¤ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•ì‹.",
                    "status_code": 500}

        analysis_result_summary = analysis_result_tuple[0] # íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìš”ì•½
        analysis_result_risk = analysis_result_tuple[1]    # íŠœí”Œì˜ ë‘ ë²ˆì§¸ ìš”ì†Œê°€ ìœ„í—˜ ë¶„ì„

        # ì´ì „ì— ì •ì˜ëœ chunk_count ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ê³„ì‚°í•©ë‹ˆë‹¤.
        # ì˜ˆ: chunk_count = len(document_chunks_raw)
        chunk_count = len(document_chunks_raw) if 'document_chunks_raw' in locals() else 0 # ë˜ëŠ” enhanced_texts ë“±

        print("[6ë‹¨ê³„ ì™„ë£Œ] í†µí•© ë¶„ì„ ì™„ë£Œ. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ í…ìŠ¤íŠ¸ ì¤€ë¹„ë¨.")

        print("ìµœì¢… ë‹µë³€")
        final_combined_summary = doc_retriever_content.format_contract_analysis_result(
            detected_contract_type, confidence, analysis_result_summary, analysis_result_risk, "í•œêµ­ì–´", chunk_count
        )


        # --- ìµœì¢… ì„±ê³µ ë°˜í™˜ (ìš”ì²­í•˜ì‹  í˜•ì‹) ---
        print("\n[í•¨ìˆ˜ ì¢…ë£Œ] 'analyze_contract_document' ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return {
            "success": True,
            "summary": final_combined_summary,
            "storage_data": storage_data, # ê¸°ì¡´ storage_data ë³€ìˆ˜ ì‚¬ìš©
            "chunk_count": chunk_count # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œ
        }

    # â˜…â˜…â˜… ë°”ê¹¥ìª½ ìµœì¢… ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ â˜…â˜…â˜…
    except APIError as e:
        print("\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] OpenAI API ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤.")
        status_code = getattr(e, 'status_code', 500)
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {status_code})"

        if getattr(e, 'code', None) == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        return {
            "success": False,
            "error": error_message,
            "status_code": status_code
        }

    except Exception as e:
        print(f"\n[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì—ëŸ¬ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤: {str(e)}")

        # ì–¸ì–´ë³„ ì˜¤ë¥˜ ë©”ì‹œì§€
        error_messages = {
            "í•œêµ­ì–´": f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "æ—¥æœ¬èª": f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "ä¸­æ–‡": f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
            "English": f"âŒ Error occurred during processing: {str(e)}",
            "EspaÃ±ol": f"âŒ Error ocurrido durante el procesamiento: {str(e)}"
        }
        
        localized_error = error_messages.get(language, error_messages["í•œêµ­ì–´"])
        
        return {
            "success": False, 
            "error": localized_error, 
            "status_code": 500
        }
