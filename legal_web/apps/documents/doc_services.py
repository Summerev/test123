# teamproject/legal_web/apps/documents/doc_services.py

from openai import OpenAI, APIError
from django.conf import settings
from django.contrib.auth.models import AnonymousUser
from sympy import im

from . import doc_retriever
from . import doc_prompt_manager

import os
import fitz
import docx
import traceback

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (settings.pyì— OPENAI_API_KEY ì„¤ì •)
client = OpenAI(api_key=settings.OPENAI_API_KEY)
qdrant_client = doc_retriever.get_qdrant_client()

def _translate_text(text: str, target_lang_name: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.
    """
    if not text or target_lang_name == "í•œêµ­ì–´":
        return text
    try:
        prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_lang_name}ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ì›ë¬¸ì˜ ì„œì‹(ì¤„ë°”ê¿ˆ, ê¸€ë¨¸ë¦¬ ê¸°í˜¸ ë“±)ì„ ìµœëŒ€í•œ ìœ ì§€í•´ì£¼ì„¸ìš”:\n\n---\n{text}"
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2, # ë²ˆì—­ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •
        )
        return response.choices[0].message.content.strip()
    except APIError as e:
        print(f"OpenAI API Error during translation: {e}")
        # API ì˜¤ë¥˜ ë°œìƒ ì‹œ, ë²ˆì—­ ì‹¤íŒ¨ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì›ë¬¸ ë°˜í™˜
        return f"({target_lang_name} ë²ˆì—­ ì‹¤íŒ¨: API ì˜¤ë¥˜) {text}"
    except Exception as e:
        print(f"Unexpected error during translation: {e}")
        return f"({target_lang_name} ë²ˆì—­ ì‹¤íŒ¨: ì‹œìŠ¤í…œ ì˜¤ë¥˜) {text}"

# ì•½ê´€ 
def analyze_terms_document(user, uploaded_file, session_id, language='ko', doc_type='terms'):
    """
    ë¬¸ì„œ ë¶„ì„ì˜ ê° ë‹¨ê³„ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥í•˜ë©° ì‹¤í–‰í•˜ê³ , ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì¬ê·€ì  ìš”ì•½ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì•ˆì •ì ì¸ ë²„ì „ì…ë‹ˆë‹¤.
    """
    try:
        print(f"[ì•½ê´€ ë¶„ì„] ì‹œì‘: {uploaded_file.name}")

        # --- 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
        document_text = doc_retriever.get_document_text(uploaded_file)
        if not document_text:
            raise ValueError("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        doc_type_name = "ì´ìš©ì•½ê´€" if doc_type == "terms" else "ê³„ì•½ì„œ"
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ (ì´ ê¸€ì ìˆ˜: {len(document_text)}ì).")

        # --- 2. ìš”ì•½ ë° ìœ„í—˜ ë¶„ì„ ---
        try:
            # --- 2-1. Map-Reduce ìš”ì•½ ---
            summary_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=4000)
            print(f"  - [2ë‹¨ê³„] ì²­í¬ ê°¯ìˆ˜: {len(summary_chunks)}")

            individual_summaries = []
            for i, chunk in enumerate(summary_chunks):
                summary_prompt = doc_prompt_manager.get_summarize_chunk_terms_prompt(chunk, doc_type_name)
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo", messages=[{"role": "user", "content": summary_prompt}],
                    max_tokens=300, temperature=0.3
                )
                individual_summaries.append(response.choices[0].message.content)
            print(f"  - [Map ë‹¨ê³„ ì™„ë£Œ] ìƒì„±í•œ ê°œë³„ ìš”ì•½ë³¸ : {len(individual_summaries)}ê°œ")

            # --- 2-2. ì¬ê·€ì  ìš”ì•½ (Reduce ë‹¨ê³„) ---
            current_summaries = individual_summaries
            while len(current_summaries) > 1:
                print(f"    - í˜„ì¬ ìš”ì•½ë³¸ {len(current_summaries)}ê°œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ë‹¤ì‹œ ìš”ì•½í•©ë‹ˆë‹¤...")
                next_level_summaries = []
                # 10ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
                for i in range(0, len(current_summaries), 10):
                    batch = current_summaries[i:i+10]
                    reduce_prompt = doc_prompt_manager.get_combine_summaries_terms_prompt(batch, doc_type_name)
                    intermediate_summary = client.chat.completions.create(
                        model="gpt-3.5-turbo", messages=[{"role": "user", "content": reduce_prompt}],
                        max_tokens=1500, temperature=0.5
                    ).choices[0].message.content
                    next_level_summaries.append(intermediate_summary)
                current_summaries = next_level_summaries
            
            final_summary_ko = current_summaries[0] if current_summaries else "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            print("  - [Reduce ë‹¨ê³„ ì™„ë£Œ] ìµœì¢… ìš”ì•½ë³¸ì„ ìƒì„±")

            # --- 2-3. ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ---
            risk_text_ko_prompt = doc_prompt_manager.get_risk_factors_terms_prompt(document_text)
            risk_text_ko = client.chat.completions.create(
                model="gpt-3.5-turbo", messages=[{"role": "user", "content": risk_text_ko_prompt}],
                max_tokens=1000, temperature=0.3
            ).choices[0].message.content
            print("  - [2ë‹¨ê³„ ì™„ë£Œ] ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì™„ë£Œ")

        except Exception as summary_e:
            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜ - 2ë‹¨ê³„] ìš”ì•½ ë˜ëŠ” ìœ„í—˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {summary_e}")
            traceback.print_exc() 
            raise summary_e

        # --- 3. ë²ˆì—­ ---
        final_summary_lang, risk_text_lang = final_summary_ko, risk_text_ko
        lang_map = {'en': 'English', 'es': 'Spanish', 'ja': 'ì¼ë³¸ì–´', 'zh': 'ì¤‘êµ­ì–´'}
        if language in lang_map:
            target_lang_name = lang_map[language]
            final_summary_lang = _translate_text(final_summary_ko, target_lang_name)
            risk_text_lang = _translate_text(risk_text_ko, target_lang_name)
        print("[3ë‹¨ê³„ ì™„ë£Œ] ë²ˆì—­ ì„±ê³µ.")

        # 4. QAë¥¼ ìœ„í•œ ë²¡í„°í™”
        qa_chunks = doc_retriever.split_text_into_chunks_terms(document_text, chunk_size=1500)
        
        if isinstance(user, AnonymousUser):
            faiss_index, indexed_chunks = doc_retriever.create_faiss_index(client, qa_chunks)
            storage_data = {"type": "faiss", "index": faiss_index, "chunks": indexed_chunks}
        else:
            doc_retriever.upsert_document_to_qdrant(qdrant_client, qa_chunks, client, user.id, session_id)
            storage_data = {"type": "qdrant"}

        # 5. ê²°ê³¼ ë°˜í™˜
        summary_text = f"ğŸ“‹ ë¬¸ì„œ ìš”ì•½\n\n{final_summary_lang}\n\n---\n\nâš ï¸ ìœ„í—˜ ìš”ì†Œ ì‹ë³„\n\n{risk_text_lang}"
        
        print(f"[ì•½ê´€ ë¶„ì„] ì™„ë£Œ: {uploaded_file.name}")
        return {
            "success": True,
            "summary": summary_text,
            "storage_data": storage_data,
            "message": "ì•½ê´€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except APIError as e:
        error_message = f"AI ëª¨ë¸ í†µì‹  ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {e.status_code})"
        if e.code == 'insufficient_quota':
            error_message = "AI ì„œë¹„ìŠ¤ ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."
        
        print(f"[ERROR] ì•½ê´€ ë¶„ì„ - OpenAI API Error: {e}")
        return {"success": False, "error": error_message}
    
    except Exception as e:
        print(f"[ìµœì¢… ì˜¤ë¥˜ ì²˜ë¦¬] ì˜ˆìƒì¹˜ ëª»í•œ ì¼ë°˜ ì˜¤ë¥˜:")
        traceback.print_exc() # ëª¨ë“  ì¢…ë¥˜ì˜ ì˜ˆì™¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ì¶œë ¥
        return {"success": False, "error": "ì„œë²„ ë‚´ë¶€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "status_code": 500}



def analyze_contract_document(user, uploaded_file, session_id, language='ko'):
    """
    ê³„ì•½ì„œ ë¬¸ì„œ ë¶„ì„: ê¸°ì¡´ ë°©ì‹ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    try:
        print(f"[ê³„ì•½ì„œ ë¶„ì„] ì‹œì‘: {uploaded_file.name}")
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        filename = uploaded_file.name
        ext = os.path.splitext(filename)[1].lower().lstrip('.')
        
        content = ''
        if ext == 'pdf':
            doc = fitz.open(stream=uploaded_file.read(), filetype='pdf')
            content = "\n".join(page.get_text() for page in doc)
        elif ext == 'docx':
            document = docx.Document(uploaded_file)
            content = "\n".join(p.text for p in document.paragraphs)
        elif ext == 'txt':
            content = uploaded_file.read().decode('utf-8')
        elif ext == 'doc':
            return {
                "success": False,
                "error": ".doc í˜•ì‹ì€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. MS Wordì—ì„œ .docxë¡œ ì €ì¥ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
            }
        else:
            return {
                "success": False,
                "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}"
            }
        
        if not content.strip():
            return {
                "success": False,
                "error": "ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        print(f"[ê³„ì•½ì„œ ë¶„ì„] ì™„ë£Œ: {uploaded_file.name} ({len(content)}ì)")
        return {
            "success": True,
            "text": content,
            "summary": f"ğŸ“„ ê³„ì•½ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ\n\në¬¸ì„œ ê¸¸ì´: {len(content)}ì\n\nì¶”ì¶œëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
            "message": "ê³„ì•½ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        }
        
    except Exception as e:
        print(f"[ERROR] ê³„ì•½ì„œ ë¶„ì„ - Error: {e}")
        return {
            "success": False,
            "error": f"ê³„ì•½ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        }
